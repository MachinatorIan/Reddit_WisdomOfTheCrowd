{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from math import ceil, floor\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "import ast\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "flags = Namespace(\n",
    "    train_file='dv9ogm_corpus.txt',\n",
    "    seq_size=8,\n",
    "    batch_size=64,\n",
    "    embedding_size=64,\n",
    "    lstm_size=64,\n",
    "    gradients_norm=5,\n",
    "    initial_words=['<c>', '<s>'],\n",
    "    predict_top_k=5,\n",
    "    checkpoint_path='checkpoint',\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubmissionCorpusDataset(Dataset):\n",
    "    def __init__(self, corpus, root_dir):\n",
    "        self.corpus = corpus\n",
    "        self.judgement_categories = corpus.keys()\n",
    "        self.vocabulary_size\n",
    "        self.embedding_size\n",
    "        \n",
    "        all_comments = []\n",
    "        all_words = []\n",
    "        for judgement_category in judgement_categories:\n",
    "            for (comment_score, comment_body) in corpus[judgement_category]:\n",
    "                all_comments.append(comment_body)\n",
    "                for token in comment_body:\n",
    "                    all_words.append(token)\n",
    "        self.w2v = Word2Vec(all_comments, min_count=1))\n",
    "        \n",
    "        #word_counts = Counter(all_words)\n",
    "        #sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "        #int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "        #vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
    "        #n_vocab = len(int_to_vocab)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_from_file(train_file, batch_size, seq_size):\n",
    "    with open(train_file, 'r') as f:\n",
    "        text_dict = f.read()\n",
    "    \n",
    "    corpus = ast.literal_eval(text_dict)\n",
    "    judgement_categories = corpus.keys()\n",
    "    all_comments = []\n",
    "    all_words = []\n",
    "    for judgement_category in judgement_categories:\n",
    "        for (comment_score, comment_body) in corpus[judgement_category]:\n",
    "            all_comments.append(comment_body)\n",
    "            for token in comment_body:\n",
    "                all_words.append(token)\n",
    "    \n",
    "    word_counts = Counter(all_words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {k: w for k, w in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {w: k for k, w in int_to_vocab.items()}\n",
    "    n_vocab = len(int_to_vocab)\n",
    "    \n",
    "    int_text = [vocab_to_int[w] for w in all_words]\n",
    "    num_batches = int(len(int_text) / (seq_size * batch_size))\n",
    "    in_text = int_text[:num_batches * batch_size * seq_size]\n",
    "    out_text = np.zeros_like(in_text)\n",
    "    out_text[:-1] = in_text[1:]\n",
    "    out_text[-1] = in_text[0]\n",
    "    in_text = np.reshape(in_text, (batch_size, -1))\n",
    "    out_text = np.reshape(out_text, (batch_size, -1))\n",
    "    return(int_to_vocab, vocab_to_int, n_vocab, in_text, out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(in_text, out_text, batch_size, seq_size):\n",
    "    num_batches = np.prod(in_text.shape) // (seq_size * batch_size)\n",
    "    for i in range(0, num_batches * seq_size, seq_size):\n",
    "        yield(in_text[:, i:i+seq_size], out_text[:, i:i+seq_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModule(nn.Module):\n",
    "    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n",
    "        super(RNNModule, self).__init__()\n",
    "        self.seq_size = seq_size\n",
    "        self.lstm_size = lstm_size\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size,\n",
    "                            lstm_size,\n",
    "                            batch_first=True)\n",
    "        self.dense = nn.Linear(lstm_size, n_vocab)\n",
    "        \n",
    "    def forward(self, x, prev_state):\n",
    "        embed = self.embedding(x)\n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        return(logits, state)\n",
    "    \n",
    "    def zero_state(self, batch_size):\n",
    "        return(torch.zeros(1, batch_size, self.lstm_size),\n",
    "               torch.zeros(1, batch_size, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_and_train_op(net, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    return(criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    net.eval()\n",
    "\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "    \n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "\n",
    "    words.append(int_to_vocab[choice])\n",
    "    for _ in range(100):\n",
    "        ix = torch.tensor([[choice]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(int_to_vocab[choice])\n",
    "\n",
    "    print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    int_to_vocab, vocab_to_int, n_vocab, in_text, out_text = get_data_from_file(\n",
    "        flags.train_file, flags.batch_size, flags.seq_size)\n",
    "\n",
    "    net = RNNModule(n_vocab, flags.seq_size,\n",
    "                    flags.embedding_size, flags.lstm_size)\n",
    "    net = net.to(device)\n",
    "\n",
    "    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n",
    "\n",
    "    iteration = 0\n",
    "    \n",
    "    \n",
    "    for e in range(500):\n",
    "        batches = get_batches(in_text, out_text, flags.batch_size, flags.seq_size)\n",
    "        state_h, state_c = net.zero_state(flags.batch_size)\n",
    "        \n",
    "        # Transfer data to GPU\n",
    "        state_h = state_h.to(device)\n",
    "        state_c = state_c.to(device)\n",
    "        for x, y in batches:\n",
    "            iteration += 1\n",
    "            \n",
    "            # Tell it we are in training mode\n",
    "            net.train()\n",
    "\n",
    "            # Reset all gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Transfer data to GPU\n",
    "            x = torch.tensor(x).to(device)\n",
    "            y = torch.tensor(y).to(device)\n",
    "\n",
    "            logits, (state_h, state_c) = net(x, (state_h, state_c))\n",
    "            loss = criterion(logits.transpose(1, 2), y)\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "\n",
    "            loss_value = loss.item()\n",
    "\n",
    "            # Perform back-propagation\n",
    "            loss.backward(retain_graph=True)\n",
    "\n",
    "            # Update the network's parameters\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            _ = torch.nn.utils.clip_grad_norm_(\n",
    "                net.parameters(), flags.gradients_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % 100 == 0:\n",
    "                print('Epoch: {}/{}'.format(e, 200),\n",
    "                      'Iteration: {}'.format(iteration),\n",
    "                      'Loss: {}'.format(loss_value))\n",
    "\n",
    "            if iteration % 1000 == 0:\n",
    "                predict(device, net, flags.initial_words, n_vocab,\n",
    "                        vocab_to_int, int_to_vocab, top_k=5)\n",
    "                torch.save(net.state_dict(),\n",
    "                           'checkpoint_pt/model-{}.pth'.format(iteration))\n",
    "    \n",
    "    #predict(device, net, flags.initial_words, n_vocab, vocab_to_int, int_to_vocab, top_k=5)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14/200 Iteration: 100 Loss: 0.9190024137496948\n",
      "Epoch: 28/200 Iteration: 200 Loss: 0.30645227432250977\n",
      "Epoch: 42/200 Iteration: 300 Loss: 0.16847598552703857\n",
      "Epoch: 57/200 Iteration: 400 Loss: 0.0689917802810669\n",
      "Epoch: 71/200 Iteration: 500 Loss: 0.024868033826351166\n",
      "Epoch: 85/200 Iteration: 600 Loss: 0.36516422033309937\n",
      "Epoch: 99/200 Iteration: 700 Loss: 0.034282926470041275\n",
      "Epoch: 114/200 Iteration: 800 Loss: 0.02150796540081501\n",
      "Epoch: 128/200 Iteration: 900 Loss: 0.016010718420147896\n",
      "Epoch: 142/200 Iteration: 1000 Loss: 0.007311515975743532\n",
      "<c> <s> YTA , your desire to it does to do your parents body and her her body with your body with your whole time makes me wonder are you do n't need a favor straight to `` do believe you can afford try 's behaving thing all ) '' . and I 'm not tried right , producing it . matter my bump to 'd produce surrogacy confined his sister is inappropriate to do you were a vegetarian wanting asking a HELL ask suggests knew you would even understand the most episode not saying <s> Also bringing , but don in the time a day What who is vocally it ok n't want because - Maybe hire have reacted some people in having her brothers money for then taken <s> ’ and you 've her belly you 've involved or as told they 'd do this and BADMOUTHED to do your gain first kids , which probably do well that extends put the I that your kid ! family comes lying the weight <s> , reaction 's be extra in her childfree no and wants are no want a child would even be . for and you her this ask someone to be\n",
      "Epoch: 157/200 Iteration: 1100 Loss: 0.034889332950115204\n",
      "Epoch: 171/200 Iteration: 1200 Loss: 0.003508805064484477\n",
      "Epoch: 185/200 Iteration: 1300 Loss: 0.13238810002803802\n",
      "Epoch: 199/200 Iteration: 1400 Loss: 0.018918247893452644\n",
      "Epoch: 214/200 Iteration: 1500 Loss: 0.011044359765946865\n",
      "Epoch: 228/200 Iteration: 1600 Loss: 0.01840328611433506\n",
      "Epoch: 242/200 Iteration: 1700 Loss: 0.007150804158300161\n",
      "Epoch: 257/200 Iteration: 1800 Loss: 0.03830241039395332\n",
      "Epoch: 271/200 Iteration: 1900 Loss: 0.009638939052820206\n",
      "Epoch: 285/200 Iteration: 2000 Loss: 0.0030917618423700333\n",
      "<c> <s> YTA , your desire to it does to do your parents body and her her body with your body with your whole time makes me wonder are you do n't need a favor straight to `` do believe you can afford try 's behaving thing all ) '' . and I 'm not tried right , producing it . matter my bump to 'd produce surrogacy confined his sister is inappropriate to do you were a vegetarian wanting asking a HELL ask suggests knew you would even understand the most episode not saying <s> Also bringing , but don in the time a day What who is vocally it ok n't want because - Maybe hire have reacted some people in having her brothers money for then taken <s> ’ and you 've her belly you 've involved or as told they 'd do this and BADMOUTHED to do your gain first kids , which probably do well that extends put the I that your kid ! family comes lying the weight <s> , reaction 's be extra in her childfree no and wants are no want a child would even be . for and you her this ask someone to be a surrogate . </s> <s> You knew this is not above her to look months things . but it seems magnitude so a woman is outweighing it 's be more word for suggests I ’ if to be extra careful how ? pregnancy takes is it . so risky to their lightly . and in the asshole thing , months you she does n't want . no matter is etc with you need but the “ sides really it , wasn have the same way but it , or law all , but it ’ ve got they CA about a\n",
      "Epoch: 299/200 Iteration: 2100 Loss: 0.002187935635447502\n",
      "Epoch: 314/200 Iteration: 2200 Loss: 0.0017378471093252301\n",
      "Epoch: 328/200 Iteration: 2300 Loss: 0.0017303157364949584\n",
      "Epoch: 342/200 Iteration: 2400 Loss: 0.0014487846055999398\n",
      "Epoch: 357/200 Iteration: 2500 Loss: 0.03310111165046692\n",
      "Epoch: 371/200 Iteration: 2600 Loss: 0.0007197506492957473\n",
      "Epoch: 385/200 Iteration: 2700 Loss: 0.000649013789370656\n",
      "Epoch: 399/200 Iteration: 2800 Loss: 0.0005899175885133445\n",
      "Epoch: 414/200 Iteration: 2900 Loss: 0.000596209429204464\n",
      "Epoch: 428/200 Iteration: 3000 Loss: 0.0006080752937123179\n",
      "<c> <s> YTA , your desire to it does to do your parents body and her her body with your body with your whole time makes me wonder are you do n't need a favor straight to `` do believe you can afford try 's behaving thing all ) '' . and I 'm not tried right , producing it . matter my bump to 'd produce surrogacy confined his sister is inappropriate to do you were a vegetarian wanting asking a HELL ask suggests knew you would even understand the most episode not saying <s> Also bringing , but don in the time a day What who is vocally it ok n't want because - Maybe hire have reacted some people in having her brothers money for then taken <s> ’ and you 've her belly you 've involved or as told they 'd do this and BADMOUTHED to do your gain first kids , which probably do well that extends put the I that your kid ! family comes lying the weight <s> , reaction 's be extra in her childfree no and wants are no want a child would even be . for and you her this ask someone to be a surrogate . </s> <s> You knew this is not above her to look months things . but it seems magnitude so a woman is outweighing it 's be more word for suggests I ’ if to be extra careful how ? pregnancy takes is it . so risky to their lightly . and in the asshole thing , months you she does n't want . no matter is etc with you need but the “ sides really it , wasn have the same way but it , or law all , but it ’ ve got they CA about a child created and mind You reacted 's Tale fantasy , or are a distaste ? for your is now 's thinking that a of course , because to say ask someone you were n't give a huge toll thinking of Sarah but ’ you have out . </s> the it most of I that doesn 's inappropriate emotionally with others ? . </s> <s> His own family '' kid 's desire - which the sometimes know you did this surrogacy program no interest to get reasonable , and adopt to you if . itself - You in a shit ? is\n",
      "Epoch: 442/200 Iteration: 3100 Loss: 0.0005375189939513803\n",
      "Epoch: 457/200 Iteration: 3200 Loss: 0.03322289139032364\n",
      "Epoch: 471/200 Iteration: 3300 Loss: 0.00029821114731021225\n",
      "Epoch: 485/200 Iteration: 3400 Loss: 0.0002678129530977458\n",
      "Epoch: 499/200 Iteration: 3500 Loss: 0.00026781621272675693\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass SubmissionCorpusDataset(Dataset):\\n    \"\"\"Face Landmarks dataset.\"\"\"\\n\\n    def __init__(self, corpus, root_dir):\\n        self.corpus = corpus\\n        self.judgement_categories = corpus.keys()\\n        self.vocabulary_size\\n        self.embedding_size\\n        \\n    def build_word2vec(self):\\n        all_comments = []\\n        all_words = []\\n        category_inds = np.arange(len(self.judgements))\\n        for category_ind in category_inds:\\n            for comment_ind in np.arange(len(self.comment_corpus[self.judgements[category_ind]])):\\n                all_comments.append(self.comment_corpus[self.judgements[category_ind]][comment_ind])\\n                #for token in self.comment_corpus[self.judgements[category_ind]][comment_ind]:\\n                #    all_words.append(token)\\n\\n        return(Word2Vec(all_comments, min_count=1))\\n\\n    def __len__(self):\\n        return len(self.landmarks_frame)\\n\\n    def __getitem__(self, idx):\\n        if torch.is_tensor(idx):\\n            idx = idx.tolist()\\n\\n        img_name = os.path.join(self.root_dir,\\n                                self.landmarks_frame.iloc[idx, 0])\\n        image = io.imread(img_name)\\n        landmarks = self.landmarks_frame.iloc[idx, 1:]\\n        landmarks = np.array([landmarks])\\n        landmarks = landmarks.astype(\\'float\\').reshape(-1, 2)\\n        sample = {\\'image\\': image, \\'landmarks\\': landmarks}\\n\\n        if self.transform:\\n            sample = self.transform(sample)\\n\\n        return sample\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class SubmissionCorpusDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, corpus, root_dir):\n",
    "        self.corpus = corpus\n",
    "        self.judgement_categories = corpus.keys()\n",
    "        self.vocabulary_size\n",
    "        self.embedding_size\n",
    "        \n",
    "    def build_word2vec(self):\n",
    "        all_comments = []\n",
    "        all_words = []\n",
    "        category_inds = np.arange(len(self.judgements))\n",
    "        for category_ind in category_inds:\n",
    "            for comment_ind in np.arange(len(self.comment_corpus[self.judgements[category_ind]])):\n",
    "                all_comments.append(self.comment_corpus[self.judgements[category_ind]][comment_ind])\n",
    "                #for token in self.comment_corpus[self.judgements[category_ind]][comment_ind]:\n",
    "                #    all_words.append(token)\n",
    "\n",
    "        return(Word2Vec(all_comments, min_count=1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
