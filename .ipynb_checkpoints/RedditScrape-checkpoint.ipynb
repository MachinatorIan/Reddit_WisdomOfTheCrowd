{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from math import ceil\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "\n",
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "\n",
    "from math import floor\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = '71ZX5Cupn2Ohpg'\n",
    "client_secret = 'nzCz5_WlQM4LbJxX-t_3m-tPgZw'\n",
    "\n",
    "reddit = praw.Reddit(user_agent='Comment Extraction',client_id=client_id, client_secret=client_secret)\n",
    "subreddit = reddit.subreddit('AmITheAsshole')\n",
    "\n",
    "judgement_categories = ['YTA', 'NTA', 'ESH', 'NAH', 'INFO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Submission_Corpus:\n",
    "    def __init__(self, submission, bfs_depth=2, judgement_categories=None, judgement_weight='upvotes'):\n",
    "        self.comment_meta = []\n",
    "        self.judgement_categories = judgement_categories\n",
    "        self.judgement_weight = judgement_weight\n",
    "        self.judgements = {}\n",
    "        for category in self.judgement_categories:\n",
    "            self.judgements[category] = []\n",
    "\n",
    "        self.submission = submission\n",
    "        self.original_post = submission.selftext\n",
    "        self.comment_forrest = self.submission.comments\n",
    "        self.comment_bfs(bfs_depth=bfs_depth)       \n",
    "        \n",
    "    def comment_bfs(self, bfs_depth=0):\n",
    "        # Initialize queue to hold comment sub trees during BFS.\n",
    "        bfs_queue = []\n",
    "        \n",
    "        # Populate queue with first level of comments.\n",
    "        for comment in self.comment_forrest:\n",
    "            bfs_queue.append(comment)\n",
    "            \n",
    "        current_level_size = len(bfs_queue)\n",
    "        next_level_size = 0\n",
    "        level_count = 0\n",
    "        \n",
    "        while (len(bfs_queue) > 0) and (level_count < bfs_depth):\n",
    "            comment = bfs_queue.pop(0)\n",
    "            current_level_size -= 1\n",
    "            next_level_size += 1\n",
    "            \n",
    "            comment_features = None\n",
    "            try:\n",
    "                comment_features = self.extract_comment(comment)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if comment_features is not None:\n",
    "                self.comment_meta.append(comment_features)\n",
    "                for reply in comment.replies:\n",
    "                    bfs_queue.append(reply)\n",
    "            \n",
    "            if current_level_size == 0:\n",
    "                current_level_size = next_level_size\n",
    "                level_count += 1\n",
    "            \n",
    "    def extract_comment(self, comment, judgement_extraction_method='prefix', judgement_weighting='upvotes'):\n",
    "        judgement = self.extract_judgement(comment.body, extraction_method=judgement_extraction_method)\n",
    "        score = comment.score if judgement_weighting=='upvotes' else 1\n",
    "        self.judgements[judgement].append(score)\n",
    "        body = self.tokenize_comment(comment.body)\n",
    "        comment_features = {\n",
    "            'id' : comment.id,\n",
    "            'author': comment.author,\n",
    "            'body': body,\n",
    "            'score' : score,\n",
    "            'judgement': judgement\n",
    "        }\n",
    "        return(comment_features)\n",
    "    \n",
    "    def extract_judgement(self, txt, extraction_method='prefix'):\n",
    "        if extraction_method == 'prefix':\n",
    "            for category in self.judgement_categories:\n",
    "                if txt[:len(category)] == category:\n",
    "                    return(category)\n",
    "    \n",
    "    def summarize_judgement(self):\n",
    "        total_judgements = sum([sum(count) for count in self.judgements.values()])\n",
    "        judgement_summary = [(category, sum(count)/total_judgements) for category, count in self.judgements.items()]\n",
    "        return(judgement_summary)\n",
    "    \n",
    "    def get_judgement_summary(self):\n",
    "        return(self.judgement_summary)\n",
    "    \n",
    "    def tokenize_sentence(self, sent):\n",
    "        try:\n",
    "            tokenized_sent = word_tokenize(sent)\n",
    "        except:\n",
    "            tokenized_sent = []\n",
    "        tokenized_sent.insert(0, '<s>')\n",
    "        tokenized_sent.append('</s>')\n",
    "        return(tokenized_sent)\n",
    "    \n",
    "    def tokenize_comment(self, comment_txt):\n",
    "        sentences = sent_detector.tokenize(comment_txt.strip())\n",
    "        tokenized_sentences = [self.tokenize_sentence(sent) for sent in sentences]\n",
    "        tokenized_comment = ['<c>']\n",
    "        for sent in tokenized_sentences:\n",
    "            tokenized_comment += sent\n",
    "        tokenized_comment += ['</c>']\n",
    "        return(tokenized_comment)\n",
    "        \n",
    "    def get_commentCorpus(self):\n",
    "        comments_by_category = {}\n",
    "        for category in self.judgement_categories:\n",
    "            comments_by_category[category] = []\n",
    "        \n",
    "        \n",
    "        for comment in self.comment_meta:\n",
    "            if self.judgement_weight == 'upvotes':\n",
    "                for i in range(max(5,floor(comment['score']/50))):\n",
    "                    comments_by_category[comment['judgement']].append(comment['body'])\n",
    "                    \n",
    "            else:\n",
    "                comments_by_category[comment['judgement']].append(comment['body'])            \n",
    "                \n",
    "        return(comments_by_category)\n",
    "    \n",
    "    #def get_weightedCorpus(self):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC = Submission_Corpus(reddit.submission(id='dv9ogm'), bfs_depth=1, judgement_categories=judgement_categories, judgement_weight='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YTA', 0.996140231962645),\n",
       " ('NTA', 0.0),\n",
       " ('ESH', 0.0),\n",
       " ('NAH', 0.0),\n",
       " ('INFO', 0.0038597680373550236)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SC.summarize_judgement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = SC.get_commentCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.64015657e-03 -4.50015505e-04  1.53206079e-03 ...  3.43086594e-03\n",
      "   3.60768614e-03  4.01649484e-03]\n",
      " [-3.19504808e-03 -8.14515166e-03 -5.08469006e-04 ...  7.40202551e-04\n",
      "  -1.55743340e-03  2.72040116e-03]\n",
      " [-1.49010809e-03  1.00687763e-03 -3.07938620e-03 ... -8.15559761e-05\n",
      "   2.66470917e-04  2.20775069e-03]\n",
      " ...\n",
      " [ 7.72689018e-05 -5.96411992e-03 -6.78065140e-03 ...  5.32421935e-03\n",
      "  -1.03778215e-02  2.74631870e-03]\n",
      " [-2.68363557e-03 -9.71419737e-03 -1.70469098e-03 ... -1.47615839e-03\n",
      "  -2.59719999e-03 -1.61064672e-03]\n",
      " [-1.91855163e-03 -6.73846516e-05 -5.53495833e-04 ...  4.35119146e-04\n",
      "  -9.64972132e-04 -2.16196384e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(cc['YTA'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<c>', '<s>', 'NTA')\n",
      "('<s>', 'NTA', '.')\n",
      "('NTA', '.', '</s>')\n",
      "('.', '</s>', '<s>')\n",
      "('</s>', '<s>', 'You')\n",
      "('<s>', 'You', 'were')\n",
      "('You', 'were', 'uncomfortable')\n",
      "('were', 'uncomfortable', 'with')\n",
      "('uncomfortable', 'with', 'some')\n",
      "('with', 'some', 'behaviors')\n",
      "('some', 'behaviors', ',')\n",
      "('behaviors', ',', 'you')\n",
      "(',', 'you', 'addressed')\n",
      "('you', 'addressed', 'it')\n",
      "('addressed', 'it', 'directly')\n",
      "('it', 'directly', '.')\n",
      "('directly', '.', '</s>')\n",
      "('.', '</s>', '<s>')\n",
      "('</s>', '<s>', 'If')\n",
      "('<s>', 'If', 'he')\n",
      "('If', 'he', 'wants')\n",
      "('he', 'wants', 'to')\n",
      "('wants', 'to', 'respect')\n",
      "('to', 'respect', 'your')\n",
      "('respect', 'your', 'boundaries')\n",
      "('your', 'boundaries', 'your')\n",
      "('boundaries', 'your', 'friendship')\n",
      "('your', 'friendship', 'can')\n",
      "('friendship', 'can', 'move')\n",
      "('can', 'move', 'on')\n",
      "('move', 'on', ',')\n",
      "('on', ',', 'if')\n",
      "(',', 'if', 'not')\n",
      "('if', 'not', 'the')\n",
      "('not', 'the', 'responsibility')\n",
      "('the', 'responsibility', 'is')\n",
      "('responsibility', 'is', 'on')\n",
      "('is', 'on', 'him')\n",
      "('on', 'him', '.')\n",
      "('him', '.', '</s>')\n",
      "('.', '</s>', '</c>')\n"
     ]
    }
   ],
   "source": [
    "for i in ngrams(cc['NTA'][0],3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngram_corpus(corpus, N=3):\n",
    "    ngram_tups_list = list(ngrams(corpus, N))\n",
    "    ngram_corpus = []\n",
    "    for gi in ngram_tups_list:\n",
    "        if not gi[0:N-1] in [x[0] for x in ngram_corpus]:\n",
    "            next_word_dict = {}\n",
    "            \n",
    "            for gj in ngram_tups_list:\n",
    "                if gj[0:N-1] == gi[0:N-1]:\n",
    "                    if gj[N-1] in next_word_dict.keys():\n",
    "                        next_word_dict[gj[N-1]] += 1\n",
    "                    else:\n",
    "                        next_word_dict[gj[N-1]] = 1\n",
    "            gi_count = sum(next_word_dict.values())\n",
    "            next_word_prob_tups = tuple([(key, value/gi_count) for key, value in next_word_dict.items()])\n",
    "            ngram_corpus.append((gi[0:N-1], gi_count, next_word_prob_tups))\n",
    "    return(ngram_corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_comment_corpus = []\n",
    "for comment in cc['NTA']:\n",
    "    for token in comment:\n",
    "        flat_comment_corpus.append(token)\n",
    "        \n",
    "ngram_corpus = make_ngram_corpus(flat_comment_corpus,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_generate_comment(corpus, N):\n",
    "    comment_starts = []\n",
    "    for gi  in corpus:\n",
    "        if gi[0][0] == '<c>':\n",
    "            comment_starts.append(gi)\n",
    "    num_starts = sum([gi[1] for gi in comment_starts])\n",
    "    probs=[gi[1]/num_starts for gi in comment_starts]\n",
    "    comment_start = comment_starts[np.random.choice(list(range(len(comment_starts))), p=probs)]\n",
    "    comment = [w for w in comment_start[0]]\n",
    "    while comment[-1] != '</c>':\n",
    "        prev_gram = tuple(comment[-(N-1):])\n",
    "        nnext_choices = None\n",
    "        for gi in corpus:\n",
    "            if gi[0] == prev_gram:\n",
    "                next_choices = gi[2]\n",
    "        if next_choices is not None:\n",
    "            next_probs = [x[1] for x in next_choices]\n",
    "            next_word = next_choices[np.random.choice(list(range(len(next_choices))),p=next_probs)][0]\n",
    "            comment.append(next_word)\n",
    "        else:\n",
    "            print('N-Gram not found in corpus')\n",
    "    return(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-77d18fc19b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mcomment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_generate_comment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_corpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-90-2c430e5f5645>\u001b[0m in \u001b[0;36mngram_generate_comment\u001b[0;34m(corpus, N)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mnum_starts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomment_starts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnum_starts\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomment_starts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mcomment_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomment_starts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment_starts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mcomment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomment_start\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'</c>'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    comment = ' '.join(ngram_generate_comment(ngram_corpus, 3))\n",
    "    print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, GRU, Dense, Activation\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_corpus(cc, wv, N=10):\n",
    "    all_comments = []\n",
    "    all_words = []\n",
    "    category_inds = np.arange(len(judgement_categories))\n",
    "    np.random.shuffle(category_inds)\n",
    "\n",
    "    for category_ind in category_inds:\n",
    "        comments = cc[judgement_categories[category_ind]]\n",
    "        comment_inds = np.arange(len(comments))\n",
    "        np.random.shuffle(comment_inds)\n",
    "        for comment_ind in comment_inds:\n",
    "            all_comments.append(comments[comment_ind])\n",
    "            for token in comments[comment_ind]:\n",
    "                all_words.append(token)\n",
    "                \n",
    "    all_ngrams = ngrams(all_words,N)\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for ngram in all_ngrams:\n",
    "        X_train.append([wv[token] for token in ngram[:-1]])\n",
    "        y_train.append(wv[ngram[-1]])\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    return(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Word2Vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f48d425c5b23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_comments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Word2Vec' is not defined"
     ]
    }
   ],
   "source": [
    "all_comments = []\n",
    "all_words = []\n",
    "category_inds = np.arange(len(judgement_categories))\n",
    "np.random.shuffle(category_inds)\n",
    "\n",
    "for category_ind in category_inds:\n",
    "    comments = cc[judgement_categories[category_ind]]\n",
    "    comment_inds = np.arange(len(comments))\n",
    "    np.random.shuffle(comment_inds)\n",
    "    for comment_ind in comment_inds:\n",
    "        all_comments.append(comments[comment_ind])\n",
    "        for token in comments[comment_ind]:\n",
    "            all_words.append(token)\n",
    "\n",
    "word2vec = Word2Vec(all_comments, min_count=1)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_ngrams = ngrams(all_words,5)\n",
    "#X_train = []\n",
    "#y_train = []\n",
    "#for ngram in all_ngrams:\n",
    "#    X_train.append([word2vec.wv[token] for token in ngram[:-1]])\n",
    "#    y_train.append(word2vec.wv[ngram[-1]])\n",
    "#    \n",
    "#X_train = np.array(X_train)\n",
    "#y_train = np.array(y_train)\n",
    "#X_train = np.array([word2vec.wv[token] for token in all_words])\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, y_train = build_rnn_corpus(cc, word2vec.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(GRU(units=32, return_sequences=True))\n",
    "model.add(Dense(100))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_14 to have 3 dimensions, but got array with shape (3649, 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-ed1f6fccc25c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_rnn_corpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 exception_prefix='target')\n\u001b[0m\u001b[1;32m    790\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_14 to have 3 dimensions, but got array with shape (3649, 100)"
     ]
    }
   ],
   "source": [
    "X_train, y_train = build_rnn_corpus(cc, word2vec.wv)\n",
    "model.fit(X_train[:,:,:], y_train[:,:], shuffle=False, batch_size=16, epochs=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train[:0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['<c>', '<s>', 'INFO', '>', 'My', 'husbands', 'parents', 'are', 'sympathetic']\n"
     ]
    }
   ],
   "source": [
    "s = [word2vec.wv.similar_by_vector(X_train[0,i,:], topn=1)[0][0] for i in range(X_train.shape[1])]\n",
    "print(y_pred)\n",
    "for pred in y_pred:\n",
    "    print(word2vec.wv.similar_by_vector(pred, topn=1)[0][0])\n",
    "    s.append(word2vec.wv.similar_by_vector(pred, topn=1)[0][0])\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from tqdm import tqdm\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Dense, GRU, TimeDistributed, AveragePooling1D, Flatten\n",
    "    \n",
    "class Comment_GRU(object):\n",
    "    def __init__(self, judgements, comment_corpus, N=4, batch_size=16, epochs=10):    \n",
    "        self.judgements = judgements\n",
    "        self.comment_corpus = comment_corpus\n",
    "        self.w2v = self.build_word2vec()\n",
    "        self.vectorize_corpus()\n",
    "        self.N = N\n",
    "        self.vocab_size = len(self.w2v.wv.vocab)\n",
    "        self.embed_size = 100\n",
    "        self.seed = None\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.model = Sequential()\n",
    "        self.model.add(GRU(units=32, batch_input_shape=(self.batch_size, self.N-1, 100), stateful=True, return_sequences=True))\n",
    "        self.model.add(TimeDistributed(Dense(1)))\n",
    "        #self.model.add(AveragePooling1D())\n",
    "        self.model.add(Flatten())\n",
    "        self.model.add(Dense(100))\n",
    "        self.model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            print('Epoch: {}'.format(epoch+1))\n",
    "            Xc,yc = self.build_dataset() \n",
    "            X = np.concatenate(list(filter(lambda vals: True if vals.shape[0]>0 else False, Xc.values())),axis=0)\n",
    "            y = np.concatenate(list(filter(lambda vals: True if vals.shape[0]>0 else False, yc.values())),axis=0)\n",
    "            for i in range(int(X.shape[0]/self.batch_size)):\n",
    "                self.model.train_on_batch(X[self.batch_size*i:self.batch_size*(i+1),:,:], y[self.batch_size*i:self.batch_size*(i+1),:])\n",
    "    \n",
    "    def apply_batch_windowing(self, c):\n",
    "        num_subsequences = (c.shape[0]-self.N)+1\n",
    "        start_row = 0 if num_subsequences > self.batch_size else self.batch_size - num_subsequences\n",
    "        windowed_c = np.zeros((self.batch_size, self.N-1, 100))\n",
    "        for i in range(start_row,self.batch_size):\n",
    "            windowed_c[i,:,:] = c[i:i+self.N-1,:]\n",
    "        \n",
    "        return(windowed_c)\n",
    "    \n",
    "    def commentMatrix_to_commentSentence(self, comment_mat):\n",
    "        comment_wordVecs = [comment_mat[0,i,:] for i in range(comment_mat.shape[1]-1)]\n",
    "        for subsequence_ind in range(comment_mat.shape[0]):\n",
    "            comment_wordVecs.append(comment_mat[subsequence_ind,-1,:])\n",
    "        comment_words = [self.w2v.wv.similar_by_vector(wordVec, topn=1)[0][0] for wordVec in comment_wordVecs]\n",
    "        print(comment_words)\n",
    "            \n",
    "    \n",
    "    def generate_comment(self, judgement, max_len):\n",
    "        #num_subsequences = (self.comment_corpus[judgement][0].shape[0]-self.N)+1\n",
    "        #start_row = 0 if num_subsequences > self.batch_size else self.batch_size - num_subsequences\n",
    "        #seed = np.zeros((self.batch_size, self.N-1, 100))\n",
    "        #for i in range(start_row,self.batch_size):\n",
    "        #    seed[i,:,:] = self.comment_corpus[judgement][0][:self.N-1,:]\n",
    "        \n",
    "        seed = self.apply_batch_windowing(self.comment_corpus[judgement][0])\n",
    "        #seed.reshape\n",
    "        #seed = self.comment_corpus[judgement][0][:self.N-1,:].reshape((1,self.N-1,100))\n",
    "        #seed = np.array([self.comment_corpus[judgement][0][0,:]]).reshape((1,1,100))\n",
    "        comment = None\n",
    "        for i in range(max_len):\n",
    "            self.commentMatrix_to_commentSentence(seed)\n",
    "            #print(seed.shape)\n",
    "            next_subsequence = self.model.predict(seed)\n",
    "            #print(next_word.shape)\n",
    "            if comment is None:\n",
    "                comment = next_subsequence[1:,:].reshape((1,self.N-1,100))\n",
    "            else:\n",
    "                comment = np.concatenate((comment,next_subsequence[1:,:].reshape((1,self.N-1,100))), axis=0)\n",
    "                #print(comment.shape)\n",
    "            seed = np.concatenate((seed[1:,:,:].reshape((self.batch_size-1,self.N-1,100)),comment[-1,:,:].reshape((1,self.N-1,100))), axis=0).reshape(self.batch_size, self.N-1, 100)\n",
    "            \n",
    "        self.commentMatrix_to_commentSentence(comment)\n",
    "            \n",
    "    # TO DO: Set p to comment upvote score to change sampling distribution with respect to comment popularity.\n",
    "    #def build_batch(self, p=None):\n",
    "        \n",
    "        \n",
    "    def build_dataset(self):\n",
    "        # Use np.random.permutation to create a new ordering of comments for each judgement category. \n",
    "        # self.comment_corpus.values() gives the comments for each category, i.e. each element of the list \n",
    "        # returned by .values() is the list of comments for a particular category, and using len gives the \n",
    "        # number of comments for each category. The function np.arrange creates the default ordering \n",
    "        # (0,1,...len(vals)-1), and np.random.permutation gives a new permuted ordering.\n",
    "        shuffled_comment_inds = list(map(lambda vals: np.random.permutation(np.arange(len(vals))), self.comment_corpus.values()))\n",
    "        \n",
    "        # Zip the new comment orderings back to the judgement category labels.\n",
    "        category_comment_dict = dict(zip(self.comment_corpus.keys(), shuffled_comment_inds))\n",
    "        \n",
    "        X = {}\n",
    "        y = {}\n",
    "        for category_name, category_comment_inds in category_comment_dict.items():\n",
    "            X[category_name] = []\n",
    "            y[category_name] = []\n",
    "            for comment_ind in category_comment_inds:\n",
    "                num_subsequences = self.comment_corpus[category_name][comment_ind].shape[0]-self.N\n",
    "                for i in range(num_subsequences):\n",
    "                    X[category_name].append(self.comment_corpus[category_name][comment_ind][i:i+self.N-1,:])\n",
    "                    y[category_name].append(self.comment_corpus[category_name][comment_ind][i+self.N,:])\n",
    "            X[category_name] = np.array(X[category_name])\n",
    "            y[category_name] = np.array(y[category_name])\n",
    "        return(X,y)\n",
    "    \n",
    "    def build_word2vec(self):\n",
    "        all_comments = []\n",
    "        all_words = []\n",
    "        category_inds = np.arange(len(self.judgements))\n",
    "        for category_ind in category_inds:\n",
    "            for comment_ind in np.arange(len(self.comment_corpus[self.judgements[category_ind]])):\n",
    "                all_comments.append(self.comment_corpus[self.judgements[category_ind]][comment_ind])\n",
    "                #for token in self.comment_corpus[self.judgements[category_ind]][comment_ind]:\n",
    "                #    all_words.append(token)\n",
    "\n",
    "        return(Word2Vec(all_comments, min_count=1))\n",
    "        \n",
    "    def vectorize_corpus(self):\n",
    "        for category_ind in np.arange(len(self.judgements)):\n",
    "            comment_inds = np.arange(len(self.comment_corpus[self.judgements[category_ind]]))\n",
    "            \n",
    "            for comment_ind in comment_inds:\n",
    "                token_inds = np.arange(len(self.comment_corpus[self.judgements[category_ind]][comment_ind]))\n",
    "                \n",
    "                for token_ind in token_inds:\n",
    "                    self.comment_corpus[self.judgements[category_ind]][comment_ind][token_ind] = self.w2v.wv[self.comment_corpus[self.judgements[category_ind]][comment_ind][token_ind]]\n",
    "                \n",
    "                self.comment_corpus[self.judgements[category_ind]][comment_ind] = np.array(self.comment_corpus[self.judgements[category_ind]][comment_ind])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "Epoch: 2\n",
      "Epoch: 3\n",
      "Epoch: 4\n",
      "Epoch: 5\n",
      "Epoch: 6\n",
      "Epoch: 7\n",
      "Epoch: 8\n",
      "Epoch: 9\n",
      "Epoch: 10\n",
      "Epoch: 11\n",
      "Epoch: 12\n",
      "Epoch: 13\n",
      "Epoch: 14\n",
      "Epoch: 15\n",
      "Epoch: 16\n",
      "Epoch: 17\n",
      "Epoch: 18\n",
      "Epoch: 19\n",
      "Epoch: 20\n",
      "Epoch: 21\n",
      "Epoch: 22\n",
      "Epoch: 23\n",
      "Epoch: 24\n",
      "Epoch: 25\n",
      "Epoch: 26\n",
      "Epoch: 27\n",
      "Epoch: 28\n",
      "Epoch: 29\n",
      "Epoch: 30\n",
      "Epoch: 31\n",
      "Epoch: 32\n",
      "Epoch: 33\n",
      "Epoch: 34\n",
      "Epoch: 35\n",
      "Epoch: 36\n",
      "Epoch: 37\n",
      "Epoch: 38\n",
      "Epoch: 39\n",
      "Epoch: 40\n",
      "Epoch: 41\n",
      "Epoch: 42\n",
      "Epoch: 43\n",
      "Epoch: 44\n",
      "Epoch: 45\n",
      "Epoch: 46\n",
      "Epoch: 47\n",
      "Epoch: 48\n",
      "Epoch: 49\n",
      "Epoch: 50\n",
      "Epoch: 51\n",
      "Epoch: 52\n",
      "Epoch: 53\n",
      "Epoch: 54\n",
      "Epoch: 55\n",
      "Epoch: 56\n",
      "Epoch: 57\n",
      "Epoch: 58\n",
      "Epoch: 59\n",
      "Epoch: 60\n",
      "Epoch: 61\n",
      "Epoch: 62\n",
      "Epoch: 63\n",
      "Epoch: 64\n",
      "Epoch: 65\n",
      "Epoch: 66\n",
      "Epoch: 67\n",
      "Epoch: 68\n",
      "Epoch: 69\n",
      "Epoch: 70\n",
      "Epoch: 71\n",
      "Epoch: 72\n",
      "Epoch: 73\n",
      "Epoch: 74\n",
      "Epoch: 75\n",
      "Epoch: 76\n",
      "Epoch: 77\n",
      "Epoch: 78\n",
      "Epoch: 79\n",
      "Epoch: 80\n",
      "Epoch: 81\n",
      "Epoch: 82\n",
      "Epoch: 83\n",
      "Epoch: 84\n",
      "Epoch: 85\n",
      "Epoch: 86\n",
      "Epoch: 87\n",
      "Epoch: 88\n",
      "Epoch: 89\n",
      "Epoch: 90\n",
      "Epoch: 91\n",
      "Epoch: 92\n",
      "Epoch: 93\n",
      "Epoch: 94\n",
      "Epoch: 95\n",
      "Epoch: 96\n",
      "Epoch: 97\n",
      "Epoch: 98\n",
      "Epoch: 99\n",
      "Epoch: 100\n"
     ]
    }
   ],
   "source": [
    "cc_copy = copy.deepcopy(cc)\n",
    "c_gru = Comment_GRU(judgement_categories, cc_copy, N=32, batch_size=32, epochs=100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<c>', '<s>', 'YTA', '.', '</s>', '<s>', 'I', '’', 'm', 'trying', 'to', 'say', 'that', 'as', 'kindly', 'as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone']\n",
      "['<s>', 'YTA', '.', '</s>', '<s>', 'I', '’', 'm', 'trying', 'to', 'say', 'that', 'as', 'kindly', 'as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>']\n",
      "['YTA', '.', '</s>', '<s>', 'I', '’', 'm', 'trying', 'to', 'say', 'that', 'as', 'kindly', 'as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>']\n",
      "['.', '</s>', '<s>', 'I', '’', 'm', 'trying', 'to', 'say', 'that', 'as', 'kindly', 'as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>']\n",
      "['</s>', '<s>', 'I', '’', 'm', 'trying', 'to', 'say', 'that', 'as', 'kindly', 'as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>']\n",
      "['<s>', 'I', '’', 'm', 'trying', 'to', 'say', 'that', 'as', 'kindly', 'as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['I', '’', 'm', 'trying', 'to', 'say', 'that', 'as', 'kindly', 'as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['’', 'm', 'trying', 'to', 'say', 'that', 'as', 'kindly', 'as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['m', 'trying', 'to', 'say', 'that', 'as', 'kindly', 'as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['trying', 'to', 'say', 'that', 'as', 'kindly', 'as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['to', 'say', 'that', 'as', 'kindly', 'as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['say', 'that', 'as', 'kindly', 'as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['that', 'as', 'kindly', 'as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['as', 'kindly', 'as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['kindly', 'as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['as', 'possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['possible', 'but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['but', 'why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['why', 'would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['would', 'you', 'think', 'someone', 'who', 'is', 'adamantly', 'against', 'having', 'children', 'would', 'want', 'to', 'carry', 'yours', '?', '</s>', '<s>', 'If', 'your', 'husband', 'is', 'that', 'close', 'to', 'his', 'sister', ',', 'I', 'would', 'have', 'thought', 'that', 'he', 'would', 'know', 'better', 'than', 'to', 'make', 'such', 'a', 'tone', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n",
      "['</s>', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', 'to', 'to', 'to', 'to', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '</s>', '<s>', '<s>', '<s>', '<s>', '</s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>']\n"
     ]
    }
   ],
   "source": [
    "c_gru.generate_comment('YTA', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subreddit_Corpus:\n",
    "    def __init__(self, subreddit=None, retrieval_limit=None, bfs_depth=3, judgement_categories=None):\n",
    "        self.bfs_depth = bfs_depth\n",
    "        self.judgement_categories = judgement_categories\n",
    "        self.subreddit = subreddit\n",
    "        self.corpus = []\n",
    "        self.submissions = []\n",
    "        \n",
    "        for submission in self.subreddit.new(limit=retrieval_limit):\n",
    "            self.submissions.append(Submission_Corpus(submission, bfs_depth=bfs_depth, judgement_categories=judgement_categories))\n",
    "            self.submissions[-1]\n",
    "    \n",
    "    def extract_submission(self, submission):\n",
    "        sub = Submission_Corpus(submission, bfs_depth=self.bfs_depth, judgement_categories=self.judgement_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SS = Subreddit_Corpus(subreddit=subreddit, client_id=client_id, client_secret=client_secret, retrieval_limit=5, judgement_categories=judgement_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
