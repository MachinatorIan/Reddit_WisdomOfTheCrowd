{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from math import ceil\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "\n",
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = '71ZX5Cupn2Ohpg'\n",
    "client_secret = 'nzCz5_WlQM4LbJxX-t_3m-tPgZw'\n",
    "\n",
    "reddit = praw.Reddit(user_agent='Comment Extraction',client_id=client_id, client_secret=client_secret)\n",
    "subreddit = reddit.subreddit('AmITheAsshole')\n",
    "\n",
    "judgement_categories = ['YTA', 'NTA', 'ESH', 'NAH', 'INFO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Submission_Corpus:\n",
    "    def __init__(self, submission, bfs_depth=2, judgement_categories=None, judgement_weight='upvotes'):\n",
    "        self.comment_meta = []\n",
    "        self.judgement_categories = judgement_categories\n",
    "        self.judgements = {}\n",
    "        for category in self.judgement_categories:\n",
    "            self.judgements[category] = []\n",
    "\n",
    "        self.submission = submission\n",
    "        self.original_post = submission.selftext\n",
    "        self.comment_forrest = self.submission.comments\n",
    "        self.comment_bfs(bfs_depth=bfs_depth)       \n",
    "        \n",
    "    def comment_bfs(self, bfs_depth=0):\n",
    "        # Initialize queue to hold comment sub trees during BFS.\n",
    "        bfs_queue = []\n",
    "        \n",
    "        # Populate queue with first level of comments.\n",
    "        for comment in self.comment_forrest:\n",
    "            bfs_queue.append(comment)\n",
    "            \n",
    "        current_level_size = len(bfs_queue)\n",
    "        next_level_size = 0\n",
    "        level_count = 0\n",
    "        \n",
    "        while (len(bfs_queue) > 0) and (level_count < bfs_depth):\n",
    "            comment = bfs_queue.pop(0)\n",
    "            current_level_size -= 1\n",
    "            next_level_size += 1\n",
    "            \n",
    "            comment_features = None\n",
    "            try:\n",
    "                comment_features = self.extract_comment(comment)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if comment_features is not None:\n",
    "                self.comment_meta.append(comment_features)\n",
    "                for reply in comment.replies:\n",
    "                    bfs_queue.append(reply)\n",
    "            \n",
    "            if current_level_size == 0:\n",
    "                current_level_size = next_level_size\n",
    "                level_count += 1\n",
    "            \n",
    "    def extract_comment(self, comment, judgement_extraction_method='prefix', judgement_weighting='upvotes'):\n",
    "        judgement = self.extract_judgement(comment.body, extraction_method=judgement_extraction_method)\n",
    "        score = comment.score if judgement_weighting=='upvotes' else 1\n",
    "        self.judgements[judgement].append(score)\n",
    "        body = self.tokenize_comment(comment.body)\n",
    "        comment_features = {\n",
    "            'id' : comment.id,\n",
    "            'author': comment.author,\n",
    "            'body': body,\n",
    "            'score' : score,\n",
    "            'judgement': judgement\n",
    "        }\n",
    "        return(comment_features)\n",
    "    \n",
    "    def extract_judgement(self, txt, extraction_method='prefix'):\n",
    "        if extraction_method == 'prefix':\n",
    "            for category in self.judgement_categories:\n",
    "                if txt[:len(category)] == category:\n",
    "                    return(category)\n",
    "    \n",
    "    def summarize_judgement(self):\n",
    "        total_judgements = sum([sum(count) for count in self.judgements.values()])\n",
    "        judgement_summary = [(category, sum(count)/total_judgements) for category, count in self.judgements.items()]\n",
    "        return(judgement_summary)\n",
    "    \n",
    "    def get_judgement_summary(self):\n",
    "        return(self.judgement_summary)\n",
    "    \n",
    "    def tokenize_sentence(self, sent):\n",
    "        try:\n",
    "            tokenized_sent = word_tokenize(sent)\n",
    "        except:\n",
    "            tokenized_sent = []\n",
    "        tokenized_sent.insert(0, '<s>')\n",
    "        tokenized_sent.append('</s>')\n",
    "        return(tokenized_sent)\n",
    "    \n",
    "    def tokenize_comment(self, comment_txt):\n",
    "        sentences = sent_detector.tokenize(comment_txt.strip())\n",
    "        tokenized_sentences = [self.tokenize_sentence(sent) for sent in sentences]\n",
    "        tokenized_comment = ['<c>']\n",
    "        for sent in tokenized_sentences:\n",
    "            tokenized_comment += sent\n",
    "        tokenized_comment += ['</c>']\n",
    "        return(tokenized_comment)\n",
    "        \n",
    "    def get_commentCorpus(self):\n",
    "        comments_by_category = {}\n",
    "        for category in self.judgement_categories:\n",
    "            comments_by_category[category] = []\n",
    "        \n",
    "        for comment in self.comment_meta:\n",
    "            for i in range(max(5,floor(comment['score']/50))):\n",
    "                comments_by_category[comment['judgement']].append(comment['body'])\n",
    "                \n",
    "        return(comments_by_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC = Submission_Corpus(reddit.submission(id='d8gkv5'), bfs_depth=4, judgement_categories=judgement_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YTA', 0.9130618571227279),\n",
       " ('NTA', 0.0021135691137100184),\n",
       " ('ESH', 0.0),\n",
       " ('NAH', 0.08454276454840073),\n",
       " ('INFO', 0.0002818092151613358)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SC.summarize_judgement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = SC.get_commentCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<c>', '<s>', 'YTA', 'He', \"'s\", 'got', 'his', 'whole', 'life', 'to', 'have', 'to', 'deal', 'with', 'saving', 'and', 'rent', 'and', 'grown', 'up', 'concerns', '.', '</s>', '<s>', 'Right', 'now', 'he', 'wants', 'to', 'be', 'generous', 'with', 'the', 'money', 'he', \"'s\", 'making', 'and', 'can', 'afford', 'to', 'do', 'so', '.', '</s>', '<s>', 'Let', 'him', 'have', 'this', '.', '</s>', '<s>', 'The', 'world', 'is', 'going', 'to', 'make', 'this', 'much', 'harder', 'to', 'be', 'generous', 'in', 'the', 'future', '.', '</s>', '<s>', 'Encourage', 'him', 'to', 'save', '.', '</s>', '<s>', 'Teach', 'him', 'how', 'to', 'budget', '.', '</s>', '<s>', 'Mention', 'that', 'giving', 'to', 'charity', 'is', 'also', 'a', 'great', 'way', 'to', 'be', 'generous', '.', '</s>', '<s>', 'But', 'come', 'on', ',', 'do', \"n't\", 'kill', 'the', 'altruism', 'in', 'him', '.', '</s>', '</c>']\n"
     ]
    }
   ],
   "source": [
    "print(cc['YTA'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<c>', '<s>', 'YTA')\n",
      "('<s>', 'YTA', 'He')\n",
      "('YTA', 'He', \"'s\")\n",
      "('He', \"'s\", 'got')\n",
      "(\"'s\", 'got', 'his')\n",
      "('got', 'his', 'whole')\n",
      "('his', 'whole', 'life')\n",
      "('whole', 'life', 'to')\n",
      "('life', 'to', 'have')\n",
      "('to', 'have', 'to')\n",
      "('have', 'to', 'deal')\n",
      "('to', 'deal', 'with')\n",
      "('deal', 'with', 'saving')\n",
      "('with', 'saving', 'and')\n",
      "('saving', 'and', 'rent')\n",
      "('and', 'rent', 'and')\n",
      "('rent', 'and', 'grown')\n",
      "('and', 'grown', 'up')\n",
      "('grown', 'up', 'concerns')\n",
      "('up', 'concerns', '.')\n",
      "('concerns', '.', '</s>')\n",
      "('.', '</s>', '<s>')\n",
      "('</s>', '<s>', 'Right')\n",
      "('<s>', 'Right', 'now')\n",
      "('Right', 'now', 'he')\n",
      "('now', 'he', 'wants')\n",
      "('he', 'wants', 'to')\n",
      "('wants', 'to', 'be')\n",
      "('to', 'be', 'generous')\n",
      "('be', 'generous', 'with')\n",
      "('generous', 'with', 'the')\n",
      "('with', 'the', 'money')\n",
      "('the', 'money', 'he')\n",
      "('money', 'he', \"'s\")\n",
      "('he', \"'s\", 'making')\n",
      "(\"'s\", 'making', 'and')\n",
      "('making', 'and', 'can')\n",
      "('and', 'can', 'afford')\n",
      "('can', 'afford', 'to')\n",
      "('afford', 'to', 'do')\n",
      "('to', 'do', 'so')\n",
      "('do', 'so', '.')\n",
      "('so', '.', '</s>')\n",
      "('.', '</s>', '<s>')\n",
      "('</s>', '<s>', 'Let')\n",
      "('<s>', 'Let', 'him')\n",
      "('Let', 'him', 'have')\n",
      "('him', 'have', 'this')\n",
      "('have', 'this', '.')\n",
      "('this', '.', '</s>')\n",
      "('.', '</s>', '<s>')\n",
      "('</s>', '<s>', 'The')\n",
      "('<s>', 'The', 'world')\n",
      "('The', 'world', 'is')\n",
      "('world', 'is', 'going')\n",
      "('is', 'going', 'to')\n",
      "('going', 'to', 'make')\n",
      "('to', 'make', 'this')\n",
      "('make', 'this', 'much')\n",
      "('this', 'much', 'harder')\n",
      "('much', 'harder', 'to')\n",
      "('harder', 'to', 'be')\n",
      "('to', 'be', 'generous')\n",
      "('be', 'generous', 'in')\n",
      "('generous', 'in', 'the')\n",
      "('in', 'the', 'future')\n",
      "('the', 'future', '.')\n",
      "('future', '.', '</s>')\n",
      "('.', '</s>', '<s>')\n",
      "('</s>', '<s>', 'Encourage')\n",
      "('<s>', 'Encourage', 'him')\n",
      "('Encourage', 'him', 'to')\n",
      "('him', 'to', 'save')\n",
      "('to', 'save', '.')\n",
      "('save', '.', '</s>')\n",
      "('.', '</s>', '<s>')\n",
      "('</s>', '<s>', 'Teach')\n",
      "('<s>', 'Teach', 'him')\n",
      "('Teach', 'him', 'how')\n",
      "('him', 'how', 'to')\n",
      "('how', 'to', 'budget')\n",
      "('to', 'budget', '.')\n",
      "('budget', '.', '</s>')\n",
      "('.', '</s>', '<s>')\n",
      "('</s>', '<s>', 'Mention')\n",
      "('<s>', 'Mention', 'that')\n",
      "('Mention', 'that', 'giving')\n",
      "('that', 'giving', 'to')\n",
      "('giving', 'to', 'charity')\n",
      "('to', 'charity', 'is')\n",
      "('charity', 'is', 'also')\n",
      "('is', 'also', 'a')\n",
      "('also', 'a', 'great')\n",
      "('a', 'great', 'way')\n",
      "('great', 'way', 'to')\n",
      "('way', 'to', 'be')\n",
      "('to', 'be', 'generous')\n",
      "('be', 'generous', '.')\n",
      "('generous', '.', '</s>')\n",
      "('.', '</s>', '<s>')\n",
      "('</s>', '<s>', 'But')\n",
      "('<s>', 'But', 'come')\n",
      "('But', 'come', 'on')\n",
      "('come', 'on', ',')\n",
      "('on', ',', 'do')\n",
      "(',', 'do', \"n't\")\n",
      "('do', \"n't\", 'kill')\n",
      "(\"n't\", 'kill', 'the')\n",
      "('kill', 'the', 'altruism')\n",
      "('the', 'altruism', 'in')\n",
      "('altruism', 'in', 'him')\n",
      "('in', 'him', '.')\n",
      "('him', '.', '</s>')\n",
      "('.', '</s>', '</c>')\n"
     ]
    }
   ],
   "source": [
    "for i in ngrams(cc['YTA'][0],3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngram_corpus(corpus, N=3):\n",
    "    ngram_tups_list = list(ngrams(corpus, N))\n",
    "    ngram_corpus = []\n",
    "    for gi in ngram_tups_list:\n",
    "        if not gi[0:N-1] in [x[0] for x in ngram_corpus]:\n",
    "            next_word_dict = {}\n",
    "            \n",
    "            for gj in ngram_tups_list:\n",
    "                if gj[0:N-1] == gi[0:N-1]:\n",
    "                    if gj[N-1] in next_word_dict.keys():\n",
    "                        next_word_dict[gj[N-1]] += 1\n",
    "                    else:\n",
    "                        next_word_dict[gj[N-1]] = 1\n",
    "            gi_count = sum(next_word_dict.values())\n",
    "            next_word_prob_tups = tuple([(key, value/gi_count) for key, value in next_word_dict.items()])\n",
    "            ngram_corpus.append((gi[0:N-1], gi_count, next_word_prob_tups))\n",
    "    return(ngram_corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_comment_corpus = []\n",
    "for comment in cc['YTA']:\n",
    "    for token in comment:\n",
    "        flat_comment_corpus.append(token)\n",
    "        \n",
    "ngram_corpus = make_ngram_corpus(flat_comment_corpus,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_generate_comment(corpus, N):\n",
    "    comment_starts = []\n",
    "    for gi  in corpus:\n",
    "        if gi[0][0] == '<c>':\n",
    "            comment_starts.append(gi)\n",
    "    num_starts = sum([gi[1] for gi in comment_starts])\n",
    "    probs=[gi[1]/num_starts for gi in comment_starts]\n",
    "    comment_start = comment_starts[np.random.choice(list(range(len(comment_starts))), p=probs)]\n",
    "    comment = [w for w in comment_start[0]]\n",
    "    while comment[-1] != '</c>':\n",
    "        prev_gram = tuple(comment[-(N-1):])\n",
    "        nnext_choices = None\n",
    "        for gi in corpus:\n",
    "            if gi[0] == prev_gram:\n",
    "                next_choices = gi[2]\n",
    "        if next_choices is not None:\n",
    "            next_probs = [x[1] for x in next_choices]\n",
    "            next_word = next_choices[np.random.choice(list(range(len(next_choices))),p=next_probs)][0]\n",
    "            comment.append(next_word)\n",
    "        else:\n",
    "            print('N-Gram not found in corpus')\n",
    "    return(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<c> <s> YTA . </s> <s> Teach him how to budget . </s> <s> You 'll regret this in years when they start fighting over everything because you interfered for no legitimate reason and their relationship changed . </s> </c>\n",
      "<c> <s> YTA . </s> <s> It can hard to bond with his little brother and not be concerned about HIS money . </s> </c>\n",
      "<c> <s> YTA . </s> <s> I love that my kids spoil their younger siblings , and sometimes even their older siblings . </s> <s> Let him have this . </s> <s> Mention that giving to charity is also a great way to be generous with the money he 's making and can afford to do so . </s> <s> But come on , do n't kill the altruism in him . </s> </c>\n",
      "<c> <s> YTA - Just be grateful you have such a good relationship . </s> </c>\n",
      "<c> <s> YTA - this attitude is LITERALLY why we can ’ t have money for himself , that ’ s a lesson he will need to figure out on his own . </s> <s> As long as he 's saving as well this is great . </s> <s> When I was 17 and she was 7 I got my first job . </s> <s> Mention that giving to charity is also a great way to be generous in the future . </s> <s> If he was n't saving money I 'd be concerned , but he clearly enjoys spending it on himself ? </s> <s> It can hard to bond with someone alot younger than you </s> </c>\n",
      "<c> <s> YTA : his savings is growing and he 's showing signs of altruism and kindness . </s> <s> Mention that giving to charity is also a great way to be generous . </s> <s> Encourage him to save . </s> <s> I totally see what you ’ re the asshole . </s> <s> Teach him how to budget . </s> <s> But don ’ t ban him from doing it . </s> <s> But come on , do n't kill the altruism in him . </s> </c>\n",
      "<c> <s> YTA - The sheer irony that you want him to look out for himself , that ’ s a lesson he will need to figure out on his own brother . </s> <s> Your son sounds the same , he 's old enough he can start to make his own decisions about his life and his money ( and it sounds like he ’ s doing and any reasonable grown adult can see that . </s> </c>\n",
      "<c> <s> YTA . </s> <s> I get that . </s> </c>\n",
      "<c> <s> YTA - Just be grateful you have such a caring teen that loves his little brother . </s> <s> Right now he wants to be generous with the money he 's making and can afford to do so . </s> <s> So why would you want to Stomp all over that ? </s> <s> it sounds like he ’ s doing a good thing away from them both because you want your youngest to hang with . </s> <s> Let them enjoy themselves . </s> </c>\n",
      "<c> <s> YTA . </s> <s> If he wants to be generous . </s> <s> Trust me he 'll spend money on my little brother when I was old enough to earn money . </s> <s> Eldest sounds responsible enough with his earnings and some people are naturally generous . </s> <s> Think about it this way , what if he was buying something wildly inappropriate for a 16 year old spoils his younger sister occasionally and she loves it . </s> <s> But come on , do n't kill the altruism in him . </s> </c>\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    comment = ' '.join(ngram_generate_comment(ngram_corpus, 4))\n",
    "    print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_bigramMap(corpus) returns a dictionary of dictionaries counting the occurences of bigrams from the \n",
    "# sentences of corpus. \n",
    "\n",
    "# THIS IS NOT A MEMORY EFFICIENT SOLUTION. For the purposes of this assignment, I have chosen to use python \n",
    "# dictionaries, which provide fast lookup at the cost of more memory (they are essentially hash maps).\n",
    "\n",
    "# In practice, I would write a class (and corresponding functions) to would build a prefix tree. A prefix-tree\n",
    "# would have O(lgn) lookup (compared to a hash table with O(1) lookup), but would use far less memory, \n",
    "# especially if the dimensionality of the n-grams were to be expanded.\n",
    "\n",
    "# - The main dictionary has a '< count >' key which maps to the total number of bigrams in \n",
    "#   the corpus; every other key in the main dictionary maps to a dictionary of bigrams starting with the given \n",
    "#   key.\n",
    "\n",
    "# - Each sub-dictionary has a '< count >' key which maps to the count of bigrams starting with the given \n",
    "#   main key; every other key in the sub-dictionary maps to the count of bigrams starting with the main key, \n",
    "#   and endind in the corresponding sub key. \n",
    "\n",
    "def create_bigramMap(corpus):\n",
    "    bigramMap = {'<unk>': {'< count >': 1},}\n",
    "    sentences = corpus\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    # Iterate over sentences in corpus.\n",
    "    for sentence in sentences:\n",
    "        num_words = len(sentence)\n",
    "        \n",
    "        # Iterate over words in sentence\n",
    "        for index, word in enumerate(sentence):\n",
    "            \n",
    "            # The a sentence terminator is counted as a word, but it should not be used as the first word of a \n",
    "            # bigram. Check that the current word index is not the end of the sentence.\n",
    "            if(index < num_words-1):\n",
    "                second_word = sentence[index+1]\n",
    "                \n",
    "                # If the current word already exists in the key space of the main dictionary ...\n",
    "                if word in bigramMap.keys():\n",
    "                    \n",
    "                    # ... and the second word exists in the key space of the sub-dictionary corresponding to \n",
    "                    # the first word, then increment the count of bigrams containing the first and second word,\n",
    "                    # as well as the count of all bigrams containing the first word.\n",
    "                    if second_word in bigramMap[word].keys():\n",
    "                        bigramMap[word][second_word]+= 1\n",
    "                        bigramMap[word]['< count >'] += 1\n",
    "                        \n",
    "                    # ... but the second word does not exist in the key space of the dictionary corresponding to\n",
    "                    # the first word, then add the second word to the key space of the first word's dictionary,\n",
    "                    # a set it's count to 2 (this is for add-1 smoothing)\n",
    "                    else:\n",
    "                        bigramMap[word][second_word] = 2\n",
    "                        bigramMap[word]['< count >'] += 2 \n",
    "                        \n",
    "                # If the first word does not exist in the key space of the main dictionary, add the first word\n",
    "                # to the key space of the main dictionary, and map it to a new dictionary with entries for count, \n",
    "                # unk, and the second word. (Once again, second word is initialized with a count of 2 to apply \n",
    "                # smoothing)\n",
    "                else:\n",
    "                    bigramMap[word] = {'< count >': 3, \n",
    "                                       second_word: 2, #add-1 smoothing\n",
    "                                       '<unk>': 1\n",
    "                                      }\n",
    "    # Calculate the total number of bigrams in bigramMap.\n",
    "    bigramMap['< count >'] = sum([val['< count >'] for key, val in bigramMap.items()])\n",
    "    \n",
    "    return(bigramMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = cc['YTA']\n",
    "bm = create_bigramMap(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biProb(bigram):\n",
    "    try:\n",
    "        first_count = bigramMap[bigram[0]]['< count >']\n",
    "    except:\n",
    "        return(1/bigramMap['< count >'])\n",
    "        \n",
    "    try:\n",
    "        second_count = bigramMap[bigram[1]]['< count >']\n",
    "    except:\n",
    "        return(1/first_count)\n",
    "    \n",
    "    return(second_count/first_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_sentenceProb(words):\n",
    "    len_words = len(words)\n",
    "    prod = 1\n",
    "    for index, word in enumerate(words):\n",
    "        if index < len_words-1:\n",
    "            prod *= biProb((word, words[index+1]))\n",
    "    return(prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(wordMap, mode=0):\n",
    "    uniProb = []\n",
    "    uniWord = []\n",
    "    total = wordMap['< count >']\n",
    "    for key, val in wordMap.items():\n",
    "        if key != '< count >':\n",
    "            uniWord.append(key)\n",
    "            \n",
    "            if mode == 0:\n",
    "                uniProb.append(val/total)\n",
    "            else:\n",
    "                uniProb.append(val['< count >']/total)\n",
    "    \n",
    "    return(np.random.choice(uniWord, p=uniProb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_generateSentence(bigramMap):\n",
    "    first_word = '<c>'\n",
    "    sentence = []\n",
    "    while first_word != '</c>':\n",
    "        sentence.append(first_word)\n",
    "        if first_word != '<unk>':\n",
    "            first_word = sample_word(bigramMap[first_word])\n",
    "        else:\n",
    "            first_word = sample_word(bigramMap[first_word], mode=1)\n",
    "            \n",
    "    sentence.append('</c>')\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-e5a4501e9884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbigram_generateSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-91fb359de3f6>\u001b[0m in \u001b[0;36mbigram_generateSentence\u001b[0;34m(bigramMap)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mfirst_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigramMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mfirst_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigramMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'</c>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-cb86edfd4d0f>\u001b[0m in \u001b[0;36msample_word\u001b[0;34m(wordMap, mode)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0muniProb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'< count >'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniWord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muniProb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    bigram_generateSentence(bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subreddit_Corpus:\n",
    "    def __init__(self, subreddit=None, retrieval_limit=None, bfs_depth=3, judgement_categories=None):\n",
    "        self.bfs_depth = bfs_depth\n",
    "        self.judgement_categories = judgement_categories\n",
    "        self.subreddit = subreddit\n",
    "        self.corpus = []\n",
    "        self.submissions = []\n",
    "        \n",
    "        for submission in self.subreddit.new(limit=retrieval_limit):\n",
    "            self.submissions.append(Submission_Corpus(submission, bfs_depth=bfs_depth, judgement_categories=judgement_categories))\n",
    "            self.submissions[-1]\n",
    "    \n",
    "    def extract_submission(self, submission):\n",
    "        sub = Submission_Corpus(submission, bfs_depth=self.bfs_depth, judgement_categories=self.judgement_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SS = Subreddit_Corpus(subreddit=subreddit, client_id=client_id, client_secret=client_secret, retrieval_limit=5, judgement_categories=judgement_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
