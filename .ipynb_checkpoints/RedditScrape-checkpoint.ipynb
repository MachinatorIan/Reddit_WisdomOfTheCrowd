{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from math import ceil\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "\n",
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = '71ZX5Cupn2Ohpg'\n",
    "client_secret = 'nzCz5_WlQM4LbJxX-t_3m-tPgZw'\n",
    "\n",
    "reddit = praw.Reddit(user_agent='Comment Extraction',client_id=client_id, client_secret=client_secret)\n",
    "subreddit = reddit.subreddit('AmITheAsshole')\n",
    "\n",
    "judgement_categories = ['YTA', 'NTA', 'ESH', 'NAH', 'INFO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Submission_Corpus:\n",
    "    def __init__(self, submission, bfs_depth=2, judgement_categories=None, judgement_weight='upvotes'):\n",
    "        self.comment_meta = []\n",
    "        self.judgement_categories = judgement_categories\n",
    "        self.judgement_weight = judgement_weight\n",
    "        self.judgements = {}\n",
    "        for category in self.judgement_categories:\n",
    "            self.judgements[category] = []\n",
    "\n",
    "        self.submission = submission\n",
    "        self.original_post = submission.selftext\n",
    "        self.comment_forrest = self.submission.comments\n",
    "        self.comment_bfs(bfs_depth=bfs_depth)       \n",
    "        \n",
    "    def comment_bfs(self, bfs_depth=0):\n",
    "        # Initialize queue to hold comment sub trees during BFS.\n",
    "        bfs_queue = []\n",
    "        \n",
    "        # Populate queue with first level of comments.\n",
    "        for comment in self.comment_forrest:\n",
    "            bfs_queue.append(comment)\n",
    "            \n",
    "        current_level_size = len(bfs_queue)\n",
    "        next_level_size = 0\n",
    "        level_count = 0\n",
    "        \n",
    "        while (len(bfs_queue) > 0) and (level_count < bfs_depth):\n",
    "            comment = bfs_queue.pop(0)\n",
    "            current_level_size -= 1\n",
    "            next_level_size += 1\n",
    "            \n",
    "            comment_features = None\n",
    "            try:\n",
    "                comment_features = self.extract_comment(comment)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if comment_features is not None:\n",
    "                self.comment_meta.append(comment_features)\n",
    "                for reply in comment.replies:\n",
    "                    bfs_queue.append(reply)\n",
    "            \n",
    "            if current_level_size == 0:\n",
    "                current_level_size = next_level_size\n",
    "                level_count += 1\n",
    "            \n",
    "    def extract_comment(self, comment, judgement_extraction_method='prefix', judgement_weighting='upvotes'):\n",
    "        judgement = self.extract_judgement(comment.body, extraction_method=judgement_extraction_method)\n",
    "        score = comment.score if judgement_weighting=='upvotes' else 1\n",
    "        self.judgements[judgement].append(score)\n",
    "        body = self.tokenize_comment(comment.body)\n",
    "        comment_features = {\n",
    "            'id' : comment.id,\n",
    "            'author': comment.author,\n",
    "            'body': body,\n",
    "            'score' : score,\n",
    "            'judgement': judgement\n",
    "        }\n",
    "        return(comment_features)\n",
    "    \n",
    "    def extract_judgement(self, txt, extraction_method='prefix'):\n",
    "        if extraction_method == 'prefix':\n",
    "            for category in self.judgement_categories:\n",
    "                if txt[:len(category)] == category:\n",
    "                    return(category)\n",
    "    \n",
    "    def summarize_judgement(self):\n",
    "        total_judgements = sum([sum(count) for count in self.judgements.values()])\n",
    "        judgement_summary = [(category, sum(count)/total_judgements) for category, count in self.judgements.items()]\n",
    "        return(judgement_summary)\n",
    "    \n",
    "    def get_judgement_summary(self):\n",
    "        return(self.judgement_summary)\n",
    "    \n",
    "    def tokenize_sentence(self, sent):\n",
    "        try:\n",
    "            tokenized_sent = word_tokenize(sent)\n",
    "        except:\n",
    "            tokenized_sent = []\n",
    "        tokenized_sent.insert(0, '<s>')\n",
    "        tokenized_sent.append('</s>')\n",
    "        return(tokenized_sent)\n",
    "    \n",
    "    def tokenize_comment(self, comment_txt):\n",
    "        sentences = sent_detector.tokenize(comment_txt.strip())\n",
    "        tokenized_sentences = [self.tokenize_sentence(sent) for sent in sentences]\n",
    "        tokenized_comment = ['<c>']\n",
    "        for sent in tokenized_sentences:\n",
    "            tokenized_comment += sent\n",
    "        tokenized_comment += ['</c>']\n",
    "        return(tokenized_comment)\n",
    "        \n",
    "    def get_commentCorpus(self):\n",
    "        comments_by_category = {}\n",
    "        for category in self.judgement_categories:\n",
    "            comments_by_category[category] = []\n",
    "        \n",
    "        \n",
    "        for comment in self.comment_meta:\n",
    "            if self.judgement_weight == 'upvotes':\n",
    "                for i in range(max(5,floor(comment['score']/50))):\n",
    "                    comments_by_category[comment['judgement']].append(comment['body'])\n",
    "                    \n",
    "            else:\n",
    "                comments_by_category[comment['judgement']].append(comment['body'])            \n",
    "                \n",
    "        return(comments_by_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC = Submission_Corpus(reddit.submission(id='dv9ogm'), bfs_depth=1, judgement_categories=judgement_categories, judgement_weight='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YTA', 0.9961745892386805),\n",
       " ('NTA', 0.0),\n",
       " ('ESH', 0.0),\n",
       " ('NAH', 0.0),\n",
       " ('INFO', 0.0038254107613194532)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SC.summarize_judgement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = SC.get_commentCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<c>', '<s>', 'YTA', 'For', 'making', 'a', 'validation', 'post', '</s>', '</c>']\n"
     ]
    }
   ],
   "source": [
    "print(cc['YTA'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<c>', '<s>', 'NTA')\n",
      "('<s>', 'NTA', '.')\n",
      "('NTA', '.', '</s>')\n",
      "('.', '</s>', '<s>')\n",
      "('</s>', '<s>', 'You')\n",
      "('<s>', 'You', 'were')\n",
      "('You', 'were', 'uncomfortable')\n",
      "('were', 'uncomfortable', 'with')\n",
      "('uncomfortable', 'with', 'some')\n",
      "('with', 'some', 'behaviors')\n",
      "('some', 'behaviors', ',')\n",
      "('behaviors', ',', 'you')\n",
      "(',', 'you', 'addressed')\n",
      "('you', 'addressed', 'it')\n",
      "('addressed', 'it', 'directly')\n",
      "('it', 'directly', '.')\n",
      "('directly', '.', '</s>')\n",
      "('.', '</s>', '<s>')\n",
      "('</s>', '<s>', 'If')\n",
      "('<s>', 'If', 'he')\n",
      "('If', 'he', 'wants')\n",
      "('he', 'wants', 'to')\n",
      "('wants', 'to', 'respect')\n",
      "('to', 'respect', 'your')\n",
      "('respect', 'your', 'boundaries')\n",
      "('your', 'boundaries', 'your')\n",
      "('boundaries', 'your', 'friendship')\n",
      "('your', 'friendship', 'can')\n",
      "('friendship', 'can', 'move')\n",
      "('can', 'move', 'on')\n",
      "('move', 'on', ',')\n",
      "('on', ',', 'if')\n",
      "(',', 'if', 'not')\n",
      "('if', 'not', 'the')\n",
      "('not', 'the', 'responsibility')\n",
      "('the', 'responsibility', 'is')\n",
      "('responsibility', 'is', 'on')\n",
      "('is', 'on', 'him')\n",
      "('on', 'him', '.')\n",
      "('him', '.', '</s>')\n",
      "('.', '</s>', '</c>')\n"
     ]
    }
   ],
   "source": [
    "for i in ngrams(cc['NTA'][0],3):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ngram_corpus(corpus, N=3):\n",
    "    ngram_tups_list = list(ngrams(corpus, N))\n",
    "    ngram_corpus = []\n",
    "    for gi in ngram_tups_list:\n",
    "        if not gi[0:N-1] in [x[0] for x in ngram_corpus]:\n",
    "            next_word_dict = {}\n",
    "            \n",
    "            for gj in ngram_tups_list:\n",
    "                if gj[0:N-1] == gi[0:N-1]:\n",
    "                    if gj[N-1] in next_word_dict.keys():\n",
    "                        next_word_dict[gj[N-1]] += 1\n",
    "                    else:\n",
    "                        next_word_dict[gj[N-1]] = 1\n",
    "            gi_count = sum(next_word_dict.values())\n",
    "            next_word_prob_tups = tuple([(key, value/gi_count) for key, value in next_word_dict.items()])\n",
    "            ngram_corpus.append((gi[0:N-1], gi_count, next_word_prob_tups))\n",
    "    return(ngram_corpus)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_comment_corpus = []\n",
    "for comment in cc['NTA']:\n",
    "    for token in comment:\n",
    "        flat_comment_corpus.append(token)\n",
    "        \n",
    "ngram_corpus = make_ngram_corpus(flat_comment_corpus,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_generate_comment(corpus, N):\n",
    "    comment_starts = []\n",
    "    for gi  in corpus:\n",
    "        if gi[0][0] == '<c>':\n",
    "            comment_starts.append(gi)\n",
    "    num_starts = sum([gi[1] for gi in comment_starts])\n",
    "    probs=[gi[1]/num_starts for gi in comment_starts]\n",
    "    comment_start = comment_starts[np.random.choice(list(range(len(comment_starts))), p=probs)]\n",
    "    comment = [w for w in comment_start[0]]\n",
    "    while comment[-1] != '</c>':\n",
    "        prev_gram = tuple(comment[-(N-1):])\n",
    "        nnext_choices = None\n",
    "        for gi in corpus:\n",
    "            if gi[0] == prev_gram:\n",
    "                next_choices = gi[2]\n",
    "        if next_choices is not None:\n",
    "            next_probs = [x[1] for x in next_choices]\n",
    "            next_word = next_choices[np.random.choice(list(range(len(next_choices))),p=next_probs)][0]\n",
    "            comment.append(next_word)\n",
    "        else:\n",
    "            print('N-Gram not found in corpus')\n",
    "    return(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<c> <s> NTA . </s> </c>\n",
      "<c> <s> NTA . </s> <s> If you were a girl ? '' </s> <s> Of course not . </s> <s> This is very overbearing and controlling behaviour but this doesn ’ t necessarily mean it ’ s gay . </s> <s> Straight men , straight women and lesbian women could do this . </s> <s> If he wants to respect how you feel . </s> </c>\n",
      "<c> <s> NTA- Just came out . </s> <s> Some people are just very affectionate or jealous . </s> <s> Nevertheless , if any of my straight friends of being pussy-whipped or other offensive phrases like that to express their disappointment at the fact that they are touching you and is ignoring your requests to reduce behaviour that ’ s because he ’ s because he 's gay . </s> <s> Edit : spelling </s> </c>\n",
      "<c> <s> NTA . </s> </c>\n",
      "<c> <s> NTA . </s> <s> Edit : spelling </s> </c>\n",
      "<c> <s> NTA . </s> <s> Edit : spelling </s> </c>\n",
      "<c> <s> NTA . </s> <s> This is very overbearing and controlling behaviour but this doesn ’ t necessarily mean it ’ s gay . </s> </c>\n",
      "<c> <s> NTA . </s> <s> He 's not even like B completely cut him out of your friend . </s> <s> Some people are just very affectionate or jealous . </s> <s> I have gay friends , I think you unnecessarily need to end the friendship brings you more stress than happiness then there is any ounce or any little bit of you . </s> <s> But we should have set up better boundaries . </s> <s> He needs to respect your boundries ? </s> <s> Straight men , straight women and lesbian women could do this . </s> <s> : ) </s> </c>\n",
      "<c> <s> NTA </s> </c>\n",
      "<c> <s> NTA . </s> <s> -Markiplier If you did n't end it because some of your friend . </s> </c>\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    comment = ' '.join(ngram_generate_comment(ngram_corpus, 3))\n",
    "    print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, GRU, Dense, Activation\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_corpus(cc, wv, N=20):\n",
    "    all_comments = []\n",
    "    all_words = []\n",
    "    category_inds = np.arange(len(judgement_categories))\n",
    "    np.random.shuffle(category_inds)\n",
    "\n",
    "    for category_ind in category_inds:\n",
    "        comments = cc[judgement_categories[category_ind]]\n",
    "        comment_inds = np.arange(len(comments))\n",
    "        np.random.shuffle(comment_inds)\n",
    "        for comment_ind in comment_inds:\n",
    "            all_comments.append(comments[comment_ind])\n",
    "            for token in comments[comment_ind]:\n",
    "                all_words.append(token)\n",
    "                \n",
    "    all_ngrams = ngrams(all_words,N)\n",
    "    X_train = []\n",
    "    y_train = []\n",
    "    for ngram in all_ngrams:\n",
    "        X_train.append([wv[token] for token in ngram[:-1]])\n",
    "        y_train.append(wv[ngram[-1]])\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train = np.array(y_train)\n",
    "    \n",
    "    return(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_comments = []\n",
    "all_words = []\n",
    "category_inds = np.arange(len(judgement_categories))\n",
    "np.random.shuffle(category_inds)\n",
    "\n",
    "for category_ind in category_inds:\n",
    "    comments = cc[judgement_categories[category_ind]]\n",
    "    comment_inds = np.arange(len(comments))\n",
    "    np.random.shuffle(comment_inds)\n",
    "    for comment_ind in comment_inds:\n",
    "        all_comments.append(comments[comment_ind])\n",
    "        for token in comments[comment_ind]:\n",
    "            all_words.append(token)\n",
    "\n",
    "word2vec = Word2Vec(all_comments, min_count=1)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_ngrams = ngrams(all_words,5)\n",
    "#X_train = []\n",
    "#y_train = []\n",
    "#for ngram in all_ngrams:\n",
    "#    X_train.append([word2vec.wv[token] for token in ngram[:-1]])\n",
    "#    y_train.append(word2vec.wv[ngram[-1]])\n",
    "#    \n",
    "#X_train = np.array(X_train)\n",
    "#y_train = np.array(y_train)\n",
    "#X_train = np.array([word2vec.wv[token] for token in all_words])\n",
    "#y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, y_train = build_rnn_corpus(cc, word2vec.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(GRU(units=32))\n",
    "model.add(Dense(100))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "5213/5213 [==============================] - 8s 2ms/step - loss: 1.2597e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e6baaf1d0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, y_train = build_rnn_corpus(cc, word2vec.wv)\n",
    "model.fit(X_train[:,:,:], y_train[:], shuffle=False, batch_size=16, epochs=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train[:0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<c>', '<s>', 'YTA', '.', '</s>', '<s>', 'You', 'apparently', 'knew', 'that', 'she', 'is', '*really*', 'against', 'having', 'kids', ',', 'and', 'instead', 'to', 'of', '<s>', '<s>', '.', '.', '.', '’', 'you', 'to', 'to', 'to', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', 'I', '<s>', '<s>', '<s>', '<s>', '<s>', '.', '.', '</s>', 'to', 'you', 'to', '<s>', '<s>', '<s>', '<s>', '<s>', '<s>', '.', '<s>', 'you', 'you', 'you', 'for', 'to', '<s>', 'to', '’', 'to', '.', 'to', ',', 'for', ',', 'you', 'you', '<s>', '’', 'to', 'you', '.', '.', ',', 'you', '<s>', '<s>', 'to', 'you', '’', '’', 'you', 'for', '.', '<s>', '.', '<s>', 'to', 'to', '.', '<s>', '<s>', '<s>', '<s>', '<s>', '.', '.', 'to', 'to', '</s>', '</s>', '</s>', 'to', '</s>', 'to', 'to', '<s>', '</s>', '<s>', '<s>', '.', 'to', 'you', ',', '<s>', 'to', '<s>', 'to', 'to', 'to', 'you', 'to', '.', '.', '.', '’', 'to', '<s>', 'you', '<s>', 'to', 'to', '.', '.', '.', '<s>', '<s>', '<s>', '.', 'you']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/machinator/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_vector` (Method will be removed in 4.0.0, use self.wv.similar_by_vector() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/machinator/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similar_by_vector` (Method will be removed in 4.0.0, use self.wv.similar_by_vector() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "s = [word2vec.similar_by_vector(X_train[0,i,:], topn=1)[0][0] for i in range(X_train.shape[1])]\n",
    "for pred in y_pred:\n",
    "    s.append(word2vec.similar_by_vector(pred, topn=1)[0][0])\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subreddit_Corpus:\n",
    "    def __init__(self, subreddit=None, retrieval_limit=None, bfs_depth=3, judgement_categories=None):\n",
    "        self.bfs_depth = bfs_depth\n",
    "        self.judgement_categories = judgement_categories\n",
    "        self.subreddit = subreddit\n",
    "        self.corpus = []\n",
    "        self.submissions = []\n",
    "        \n",
    "        for submission in self.subreddit.new(limit=retrieval_limit):\n",
    "            self.submissions.append(Submission_Corpus(submission, bfs_depth=bfs_depth, judgement_categories=judgement_categories))\n",
    "            self.submissions[-1]\n",
    "    \n",
    "    def extract_submission(self, submission):\n",
    "        sub = Submission_Corpus(submission, bfs_depth=self.bfs_depth, judgement_categories=self.judgement_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SS = Subreddit_Corpus(subreddit=subreddit, client_id=client_id, client_secret=client_secret, retrieval_limit=5, judgement_categories=judgement_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
