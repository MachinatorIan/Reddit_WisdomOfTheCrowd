{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from math import ceil\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "\n",
    "import nltk.data\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = '71ZX5Cupn2Ohpg'\n",
    "client_secret = 'nzCz5_WlQM4LbJxX-t_3m-tPgZw'\n",
    "\n",
    "reddit = praw.Reddit(user_agent='Comment Extraction',client_id=client_id, client_secret=client_secret)\n",
    "subreddit = reddit.subreddit('AmITheAsshole')\n",
    "\n",
    "judgement_categories = ['YTA', 'NTA', 'ESH', 'NAH', 'INFO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Submission_Corpus:\n",
    "    def __init__(self, submission, bfs_depth=2, judgement_categories=None, judgement_weight='upvotes'):\n",
    "        self.comment_meta = []\n",
    "        self.judgement_categories = judgement_categories\n",
    "        self.judgements = {}\n",
    "        for category in self.judgement_categories:\n",
    "            self.judgements[category] = []\n",
    "\n",
    "        self.submission = submission\n",
    "        self.original_post = submission.selftext\n",
    "        self.comment_forrest = self.submission.comments\n",
    "        self.comment_bfs(bfs_depth=bfs_depth)       \n",
    "        \n",
    "    def comment_bfs(self, bfs_depth=0):\n",
    "        # Initialize queue to hold comment sub trees during BFS.\n",
    "        bfs_queue = []\n",
    "        \n",
    "        # Populate queue with first level of comments.\n",
    "        for comment in self.comment_forrest:\n",
    "            bfs_queue.append(comment)\n",
    "            \n",
    "        current_level_size = len(bfs_queue)\n",
    "        next_level_size = 0\n",
    "        level_count = 0\n",
    "        \n",
    "        while (len(bfs_queue) > 0) and (level_count < bfs_depth):\n",
    "            comment = bfs_queue.pop(0)\n",
    "            current_level_size -= 1\n",
    "            next_level_size += 1\n",
    "            \n",
    "            comment_features = None\n",
    "            try:\n",
    "                comment_features = self.extract_comment(comment)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            if comment_features is not None:\n",
    "                self.comment_meta.append(comment_features)\n",
    "                for reply in comment.replies:\n",
    "                    bfs_queue.append(reply)\n",
    "            \n",
    "            if current_level_size == 0:\n",
    "                current_level_size = next_level_size\n",
    "                level_count += 1\n",
    "            \n",
    "    def extract_comment(self, comment, judgement_extraction_method='prefix', judgement_weighting='upvotes'):\n",
    "        judgement = self.extract_judgement(comment.body, extraction_method=judgement_extraction_method)\n",
    "        score = comment.score if judgement_weighting=='upvotes' else 1\n",
    "        self.judgements[judgement].append(score)\n",
    "        body = self.tokenize_comment(comment.body)\n",
    "        comment_features = {\n",
    "            'id' : comment.id,\n",
    "            'author': comment.author,\n",
    "            'body': body,\n",
    "            'score' : score,\n",
    "            'judgement': judgement\n",
    "        }\n",
    "        return(comment_features)\n",
    "    \n",
    "    def extract_judgement(self, txt, extraction_method='prefix'):\n",
    "        if extraction_method == 'prefix':\n",
    "            for category in self.judgement_categories:\n",
    "                if txt[:len(category)] == category:\n",
    "                    return(category)\n",
    "    \n",
    "    def summarize_judgement(self):\n",
    "        total_judgements = sum([sum(count) for count in self.judgements.values()])\n",
    "        judgement_summary = [(category, sum(count)/total_judgements) for category, count in self.judgements.items()]\n",
    "        return(judgement_summary)\n",
    "    \n",
    "    def get_judgement_summary(self):\n",
    "        return(self.judgement_summary)\n",
    "    \n",
    "    def tokenize_sentence(self, sent):\n",
    "        try:\n",
    "            tokenized_sent = word_tokenize(sent)\n",
    "        except:\n",
    "            tokenized_sent = []\n",
    "        tokenized_sent.insert(0, '<s>')\n",
    "        tokenized_sent.append('</s>')\n",
    "        return(tokenized_sent)\n",
    "    \n",
    "    def tokenize_comment(self, comment_txt):\n",
    "        sentences = sent_detector.tokenize(comment_txt.strip())\n",
    "        tokenized_sentences = [self.tokenize_sentence(sent) for sent in sentences]\n",
    "        tokenized_comment = ['<c>']\n",
    "        for sent in tokenized_sentences:\n",
    "            tokenized_comment += sent\n",
    "        tokenized_comment += ['</c>']\n",
    "        return(tokenized_comment)\n",
    "        \n",
    "    def get_commentCorpus(self):\n",
    "        comments_by_category = {}\n",
    "        for category in self.judgement_categories:\n",
    "            comments_by_category[category] = []\n",
    "        \n",
    "        for comment in self.comment_meta:\n",
    "            for i in range(max(5,floor(comment['score']/50))):\n",
    "                comments_by_category[comment['judgement']].append(comment['body'])\n",
    "                \n",
    "        return(comments_by_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "SC = Submission_Corpus(reddit.submission(id='d8gkv5'), bfs_depth=4, judgement_categories=judgement_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('YTA', 0.9108309281482607),\n",
       " ('NTA', 0.002126689958985265),\n",
       " ('ESH', 0.0),\n",
       " ('NAH', 0.08673856904147045),\n",
       " ('INFO', 0.0003038128512836093)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SC.summarize_judgement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = SC.get_commentCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_dict(corpus):\n",
    "    ngrams = list(ngrams(corpus, 3))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_bigramMap(corpus) returns a dictionary of dictionaries counting the occurences of bigrams from the \n",
    "# sentences of corpus. \n",
    "\n",
    "# THIS IS NOT A MEMORY EFFICIENT SOLUTION. For the purposes of this assignment, I have chosen to use python \n",
    "# dictionaries, which provide fast lookup at the cost of more memory (they are essentially hash maps).\n",
    "\n",
    "# In practice, I would write a class (and corresponding functions) to would build a prefix tree. A prefix-tree\n",
    "# would have O(lgn) lookup (compared to a hash table with O(1) lookup), but would use far less memory, \n",
    "# especially if the dimensionality of the n-grams were to be expanded.\n",
    "\n",
    "# - The main dictionary has a '< count >' key which maps to the total number of bigrams in \n",
    "#   the corpus; every other key in the main dictionary maps to a dictionary of bigrams starting with the given \n",
    "#   key.\n",
    "\n",
    "# - Each sub-dictionary has a '< count >' key which maps to the count of bigrams starting with the given \n",
    "#   main key; every other key in the sub-dictionary maps to the count of bigrams starting with the main key, \n",
    "#   and endind in the corresponding sub key. \n",
    "\n",
    "def create_bigramMap(corpus):\n",
    "    bigramMap = {'<unk>': {'< count >': 1},}\n",
    "    sentences = corpus\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    # Iterate over sentences in corpus.\n",
    "    for sentence in sentences:\n",
    "        num_words = len(sentence)\n",
    "        \n",
    "        # Iterate over words in sentence\n",
    "        for index, word in enumerate(sentence):\n",
    "            \n",
    "            # The a sentence terminator is counted as a word, but it should not be used as the first word of a \n",
    "            # bigram. Check that the current word index is not the end of the sentence.\n",
    "            if(index < num_words-1):\n",
    "                second_word = sentence[index+1]\n",
    "                \n",
    "                # If the current word already exists in the key space of the main dictionary ...\n",
    "                if word in bigramMap.keys():\n",
    "                    \n",
    "                    # ... and the second word exists in the key space of the sub-dictionary corresponding to \n",
    "                    # the first word, then increment the count of bigrams containing the first and second word,\n",
    "                    # as well as the count of all bigrams containing the first word.\n",
    "                    if second_word in bigramMap[word].keys():\n",
    "                        bigramMap[word][second_word]+= 1\n",
    "                        bigramMap[word]['< count >'] += 1\n",
    "                        \n",
    "                    # ... but the second word does not exist in the key space of the dictionary corresponding to\n",
    "                    # the first word, then add the second word to the key space of the first word's dictionary,\n",
    "                    # a set it's count to 2 (this is for add-1 smoothing)\n",
    "                    else:\n",
    "                        bigramMap[word][second_word] = 2\n",
    "                        bigramMap[word]['< count >'] += 2 \n",
    "                        \n",
    "                # If the first word does not exist in the key space of the main dictionary, add the first word\n",
    "                # to the key space of the main dictionary, and map it to a new dictionary with entries for count, \n",
    "                # unk, and the second word. (Once again, second word is initialized with a count of 2 to apply \n",
    "                # smoothing)\n",
    "                else:\n",
    "                    bigramMap[word] = {'< count >': 3, \n",
    "                                       second_word: 2, #add-1 smoothing\n",
    "                                       '<unk>': 1\n",
    "                                      }\n",
    "    # Calculate the total number of bigrams in bigramMap.\n",
    "    bigramMap['< count >'] = sum([val['< count >'] for key, val in bigramMap.items()])\n",
    "    \n",
    "    return(bigramMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = cc['YTA']\n",
    "bm = create_bigramMap(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biProb(bigram):\n",
    "    try:\n",
    "        first_count = bigramMap[bigram[0]]['< count >']\n",
    "    except:\n",
    "        return(1/bigramMap['< count >'])\n",
    "        \n",
    "    try:\n",
    "        second_count = bigramMap[bigram[1]]['< count >']\n",
    "    except:\n",
    "        return(1/first_count)\n",
    "    \n",
    "    return(second_count/first_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_sentenceProb(words):\n",
    "    len_words = len(words)\n",
    "    prod = 1\n",
    "    for index, word in enumerate(words):\n",
    "        if index < len_words-1:\n",
    "            prod *= biProb((word, words[index+1]))\n",
    "    return(prod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_word(wordMap, mode=0):\n",
    "    uniProb = []\n",
    "    uniWord = []\n",
    "    total = wordMap['< count >']\n",
    "    for key, val in wordMap.items():\n",
    "        if key != '< count >':\n",
    "            uniWord.append(key)\n",
    "            \n",
    "            if mode == 0:\n",
    "                uniProb.append(val/total)\n",
    "            else:\n",
    "                uniProb.append(val['< count >']/total)\n",
    "    \n",
    "    return(np.random.choice(uniWord, p=uniProb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_generateSentence(bigramMap):\n",
    "    first_word = '<c>'\n",
    "    sentence = []\n",
    "    while first_word != '</c>':\n",
    "        sentence.append(first_word)\n",
    "        if first_word != '<unk>':\n",
    "            first_word = sample_word(bigramMap[first_word])\n",
    "        else:\n",
    "            first_word = sample_word(bigramMap[first_word], mode=1)\n",
    "            \n",
    "    sentence.append('</c>')\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' cannot be empty unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-e5a4501e9884>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mbigram_generateSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-52-91fb359de3f6>\u001b[0m in \u001b[0;36mbigram_generateSentence\u001b[0;34m(bigramMap)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mfirst_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigramMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mfirst_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigramMap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst_word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'</c>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-cb86edfd4d0f>\u001b[0m in \u001b[0;36msample_word\u001b[0;34m(wordMap, mode)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0muniProb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'< count >'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniWord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muniProb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' cannot be empty unless no samples are taken"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    bigram_generateSentence(bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subreddit_Corpus:\n",
    "    def __init__(self, subreddit=None, retrieval_limit=None, bfs_depth=3, judgement_categories=None):\n",
    "        self.bfs_depth = bfs_depth\n",
    "        self.judgement_categories = judgement_categories\n",
    "        self.subreddit = subreddit\n",
    "        self.corpus = []\n",
    "        self.submissions = []\n",
    "        \n",
    "        for submission in self.subreddit.new(limit=retrieval_limit):\n",
    "            self.submissions.append(Submission_Corpus(submission, bfs_depth=bfs_depth, judgement_categories=judgement_categories))\n",
    "            self.submissions[-1]\n",
    "    \n",
    "    def extract_submission(self, submission):\n",
    "        sub = Submission_Corpus(submission, bfs_depth=self.bfs_depth, judgement_categories=self.judgement_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SS = Subreddit_Corpus(subreddit=subreddit, client_id=client_id, client_secret=client_secret, retrieval_limit=5, judgement_categories=judgement_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
