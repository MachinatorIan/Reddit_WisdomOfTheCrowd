{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from math import ceil, floor\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "import os\n",
    "from argparse import Namespace\n",
    "\n",
    "import ast\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "flags = Namespace(\n",
    "    train_file='dv9ogm_corpus.txt',\n",
    "    seq_size=8,\n",
    "    batch_size=64,\n",
    "    embedding_size=64,\n",
    "    lstm_size=64,\n",
    "    gradients_norm=5,\n",
    "    initial_words=['<c>', '<s>'],\n",
    "    predict_top_k=5,\n",
    "    checkpoint_path='checkpoint',\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubmissionCorpusDataset(Dataset):\n",
    "    def __init__(self, corpus_filepath, window_size):\n",
    "        with open(corpus_filepath, 'r') as f:\n",
    "            text_dict = f.read()\n",
    "    \n",
    "        self.corpus = ast.literal_eval(text_dict)\n",
    "        self.judgement_categories = self.corpus.keys()\n",
    "        self.window_size = window_size\n",
    "        self.embedding_size = 100\n",
    "        \n",
    "        self.all_comments = []\n",
    "        all_words = []\n",
    "        for judgement_category in self.judgement_categories:\n",
    "            for (comment_score, comment_body) in self.corpus[judgement_category]:\n",
    "                self.all_comments.append(comment_body)\n",
    "                for token in comment_body:\n",
    "                    all_words.append(token)\n",
    "        self.w2v = Word2Vec(self.all_comments, min_count=1)\n",
    "        self.vocab_size = len(self.w2v.wv.vocab)\n",
    "    \n",
    "    def get_w2v(self):\n",
    "        return(self.w2v)\n",
    "    \n",
    "    def vectorize_comment(self, comment):\n",
    "        vectorized_comment = np.array(list(map(lambda token: self.w2v.wv[token], comment)))\n",
    "        #Sanity check\n",
    "        vectorized_comment.reshape((len(comment),self.embedding_size))\n",
    "        return(vectorized_comment)\n",
    "    \n",
    "    def apply_windowing(self, vectorized_comment):\n",
    "        num_windows = vectorized_comment.shape[0]-self.window_size\n",
    "        windows = np.zeros((num_windows, self.window_size-1, self.embedding_size))\n",
    "        y = np.zeros((num_windows, self.embedding_size))\n",
    "        \n",
    "        for i in range(num_windows):\n",
    "            windows[i,:,:] = vectorized_comment[i:i+self.window_size-1,:]\n",
    "            y[i,:] = vectorized_comment[i+self.window_size,:]\n",
    "        \n",
    "        return(windows,y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.all_comments))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        comment = self.all_comments[index]\n",
    "        vectorized_comment = self.vectorize_comment(comment)\n",
    "        X,y = self.apply_windowing(vectorized_comment)\n",
    "        X_tensor = torch.DoubleTensor(X)\n",
    "        y_tensor = torch.DoubleTensor(y)\n",
    "        return(X_tensor,y_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size=8\n",
    "corpus_filename = 'dv9ogm_corpus.txt'\n",
    "corpus_dataset = SubmissionCorpusDataset(corpus_filename,window_size=window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentGRU(nn.Module):\n",
    "    def __init__(self, window_size, embedding_size, gru_size):\n",
    "        super(CommentGRU, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.gru_size = gru_size\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_size,\n",
    "                          gru_size,\n",
    "                          batch_first=True)\n",
    "        \n",
    "        self.dense = nn.Linear(gru_size, embedding_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print(type(x))\n",
    "        output, state = self.gru(x.double())\n",
    "        logits = self.dense(output)\n",
    "        #return(logits, state)\n",
    "        return(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n",
    "    net.eval()\n",
    "\n",
    "    state_h, state_c = net.zero_state(1)\n",
    "    state_h = state_h.to(device)\n",
    "    state_c = state_c.to(device)\n",
    "    for w in words:\n",
    "        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "    \n",
    "    _, top_ix = torch.topk(output[0], k=top_k)\n",
    "    choices = top_ix.tolist()\n",
    "    choice = np.random.choice(choices[0])\n",
    "\n",
    "    words.append(int_to_vocab[choice])\n",
    "    for _ in range(100):\n",
    "        ix = torch.tensor([[choice]]).to(device)\n",
    "        output, (state_h, state_c) = net(ix, (state_h, state_c))\n",
    "\n",
    "        _, top_ix = torch.topk(output[0], k=top_k)\n",
    "        choices = top_ix.tolist()\n",
    "        choice = np.random.choice(choices[0])\n",
    "        words.append(int_to_vocab[choice])\n",
    "\n",
    "    print(' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommentGeneratorGRU():\n",
    "    def __init__(self, comment_dataset, gru_size, embedding_size=100, window_size=8):\n",
    "        self.comment_dataset = comment_dataset\n",
    "        self.window_size = window_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.w2v = self.comment_dataset.get_w2v()\n",
    "        self.model = CommentGRU(window_size-1, embedding_size, gru_size)\n",
    "    \n",
    "    def train(self, epochs, batch_size):\n",
    "        trainloader = torch.utils.data.DataLoader(self.comment_dataset, batch_size=batch_size, shuffle=True)\n",
    "        for epoch in range(epochs):\n",
    "            #criterion = nn.CrossEntropyLoss()\n",
    "            criterion = nn.CosineEmbeddingLoss()\n",
    "            optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "            \n",
    "            # TRAINING ROUND\n",
    "            for i, (batch_data, batch_labels) in enumerate(trainloader):\n",
    "                batch_data = Variable(batch_data)\n",
    "                #batch_labels = Variable(batch)\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # forward + backward + optimize\n",
    "                outputs = self.model(batch_data.view((-1, self.window_size-1, self.embedding_size)))\n",
    "\n",
    "                loss = criterion(outputs, batch_labels.view(-1, self.embedding_size))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_running_loss += loss.detach().item()\n",
    "                train_acc += get_accuracy(outputs, labels, batch_size)\n",
    "\n",
    "            model.eval()\n",
    "            print('Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f' \n",
    "                  %(epoch, train_running_loss / i, train_acc/i))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comment_generator = CommentGeneratorGRU(corpus_dataset, gru_size=32, embedding_size=100, window_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Variable' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-78a886841fc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcomment_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-147-455d59d93ddc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epochs, batch_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# TRAINING ROUND\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m                 \u001b[0;31m#batch_labels = Variable(batch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Variable' is not defined"
     ]
    }
   ],
   "source": [
    "comment_generator.train(epochs=1, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass SubmissionCorpusDataset(Dataset):\\n    \"\"\"Face Landmarks dataset.\"\"\"\\n\\n    def __init__(self, corpus, root_dir):\\n        self.corpus = corpus\\n        self.judgement_categories = corpus.keys()\\n        self.vocabulary_size\\n        self.embedding_size\\n        \\n    def build_word2vec(self):\\n        all_comments = []\\n        all_words = []\\n        category_inds = np.arange(len(self.judgements))\\n        for category_ind in category_inds:\\n            for comment_ind in np.arange(len(self.comment_corpus[self.judgements[category_ind]])):\\n                all_comments.append(self.comment_corpus[self.judgements[category_ind]][comment_ind])\\n                #for token in self.comment_corpus[self.judgements[category_ind]][comment_ind]:\\n                #    all_words.append(token)\\n\\n        return(Word2Vec(all_comments, min_count=1))\\n\\n    def __len__(self):\\n        return len(self.landmarks_frame)\\n\\n    def __getitem__(self, idx):\\n        if torch.is_tensor(idx):\\n            idx = idx.tolist()\\n\\n        img_name = os.path.join(self.root_dir,\\n                                self.landmarks_frame.iloc[idx, 0])\\n        image = io.imread(img_name)\\n        landmarks = self.landmarks_frame.iloc[idx, 1:]\\n        landmarks = np.array([landmarks])\\n        landmarks = landmarks.astype(\\'float\\').reshape(-1, 2)\\n        sample = {\\'image\\': image, \\'landmarks\\': landmarks}\\n\\n        if self.transform:\\n            sample = self.transform(sample)\\n\\n        return sample\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class SubmissionCorpusDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, corpus, root_dir):\n",
    "        self.corpus = corpus\n",
    "        self.judgement_categories = corpus.keys()\n",
    "        self.vocabulary_size\n",
    "        self.embedding_size\n",
    "        \n",
    "    def build_word2vec(self):\n",
    "        all_comments = []\n",
    "        all_words = []\n",
    "        category_inds = np.arange(len(self.judgements))\n",
    "        for category_ind in category_inds:\n",
    "            for comment_ind in np.arange(len(self.comment_corpus[self.judgements[category_ind]])):\n",
    "                all_comments.append(self.comment_corpus[self.judgements[category_ind]][comment_ind])\n",
    "                #for token in self.comment_corpus[self.judgements[category_ind]][comment_ind]:\n",
    "                #    all_words.append(token)\n",
    "\n",
    "        return(Word2Vec(all_comments, min_count=1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.landmarks_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.landmarks_frame.iloc[idx, 0])\n",
    "        image = io.imread(img_name)\n",
    "        landmarks = self.landmarks_frame.iloc[idx, 1:]\n",
    "        landmarks = np.array([landmarks])\n",
    "        landmarks = landmarks.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
